{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":99552,"databundleVersionId":13441085,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**IMPORTS AND TPU SET UP**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport json\nimport warnings\nimport logging\nfrom pathlib import Path\nfrom collections import Counter\nfrom typing import Dict, List, Tuple, Optional\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport pydicom\nfrom scipy import ndimage\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, \n    roc_auc_score, confusion_matrix, classification_report,\n    roc_curve, precision_recall_curve\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport albumentations as A\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom tqdm import tqdm\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\n\nwarnings.filterwarnings('ignore')\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    TPU_AVAILABLE = xm.xla_device_hw(xm.xla_device()) in ('TPU', 'GPU')\n    if TPU_AVAILABLE:\n        print(f\"TPU detected: {xm.xla_device()}\")\n        os.environ['XLA_USE_BF16'] = '1'\n        os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\nexcept ImportError:\n    TPU_AVAILABLE = False\n    xm = None\n    print(\"TPU libraries not available, using CPU/GPU\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TPU OPTMIZED CONFIGURATION AND TRAINING PIPELINE**","metadata":{}},{"cell_type":"code","source":"class Config:\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    \n    # Model parameters\n    TARGET_SIZE = (16, 128, 128) \n    BATCH_SIZE = 4 if TPU_AVAILABLE else 8\n    EPOCHS = 20\n    LEARNING_RATE = 1e-4  \n    WEIGHT_DECAY = 1e-4\n    WARMUP_EPOCHS = 2\n    \n    # Training parameters\n    GRADIENT_CLIP = 0.5  \n    ACCUMULATION_STEPS = 2\n    EARLY_STOPPING_PATIENCE = 5\n    SCHEDULER_PATIENCE = 3\n    \n    # Memory management\n    MAX_CACHE_SIZE = 50  \n    PREFETCH_FACTOR = 2\n    \n    @staticmethod\n    def get_device():\n        if TPU_AVAILABLE:\n            device = xm.xla_device()\n            print(f\"Using TPU device: {device}\")\n            return device\n        elif torch.cuda.is_available():\n            device = torch.device('cuda')\n            torch.cuda.empty_cache()\n            print(f\"Using CUDA device: {device}\")\n            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n            return device\n        else:\n            device = torch.device('cpu')\n            print(f\"Using CPU device: {device}\")\n            return device\n    \n    DEVICE = get_device()\n    \n    # Data parameters\n    ID_COL = 'SeriesInstanceUID'\n    TARGET_COL = 'Aneurysm Present'\n    \n    # Debug settings\n    DEBUG_MODE = True\n    DEBUG_SAMPLES = 200 \n    \n    # Cross-validation - SINGLE FOLD for debugging\n    N_FOLDS = 5\n    CURRENT_FOLD = 0  \n    \n    # Data augmentation \n    USE_AUGMENTATION = True\n    AUGMENTATION_PROB = 0.2\n\nclass OptimizedDICOMProcessor:\n    def __init__(self, target_size=None, hu_window=(-1000, 1000)):\n        self.target_size = target_size or Config.TARGET_SIZE\n        self.max_slices = self.target_size[0]\n        self.hu_window = hu_window\n        self.stats = {'processed': 0, 'failed': 0, 'dummy': 0}\n        self.cache = {}  # Memory cache for processed volumes\n        \n    def load_dicom_series(self, series_path):\n        try:\n            if series_path in self.cache:\n                self.stats['processed'] += 1\n                return self.cache[series_path].copy()\n            \n            if not os.path.exists(series_path):\n                logger.warning(f\"Series path does not exist: {series_path}\")\n                return self._get_dummy_volume()\n            \n            dicom_files = [f for f in os.listdir(series_path) \n                          if f.lower().endswith('.dcm')][:self.max_slices]\n            \n            if not dicom_files:\n                logger.warning(f\"No DICOM files found in: {series_path}\")\n                return self._get_dummy_volume()\n            \n            pixel_arrays = []\n            target_shape = self.target_size[1:]\n            \n            for file_name in dicom_files:\n                try:\n                    file_path = os.path.join(series_path, file_name)\n                    ds = pydicom.dcmread(file_path, force=True)\n                    \n                    if hasattr(ds, 'pixel_array'):\n                        arr = ds.pixel_array.astype(np.float32)\n                        \n                        if arr.ndim == 2:\n                            arr = self._simple_preprocess(arr, target_shape)\n                            pixel_arrays.append(arr)\n                            \n                        del ds\n                        \n                        if len(pixel_arrays) >= self.max_slices:\n                            break\n                            \n                except Exception as e:\n                    logger.warning(f\"Failed to load {file_name}: {e}\")\n                    continue\n            \n            if not pixel_arrays:\n                return self._get_dummy_volume()\n            \n            # Volume\n            volume = self._create_volume_efficiently(pixel_arrays)\n            \n            # Cache\n            if len(self.cache) < Config.MAX_CACHE_SIZE:\n                self.cache[series_path] = volume.copy()\n            \n            self.stats['processed'] += 1\n            return volume\n            \n        except Exception as e:\n            logger.error(f\"Error processing {series_path}: {e}\")\n            self.stats['failed'] += 1\n            return self._get_dummy_volume()\n    \n    def _simple_preprocess(self, arr, target_shape):\n        # Resize\n        if arr.shape != target_shape:\n            arr = cv2.resize(arr, (target_shape[1], target_shape[0]), \n                           interpolation=cv2.INTER_AREA)\n        \n        # Simple normalization\n        arr = np.clip(arr, *self.hu_window)\n        if arr.max() > arr.min():\n            arr = (arr - arr.min()) / (arr.max() - arr.min())\n        \n        return arr.astype(np.float32)\n    \n    def _create_volume_efficiently(self, pixel_arrays):\n        # Pad or truncate to exact size\n        if len(pixel_arrays) < self.max_slices:\n            # Pad with last slice\n            while len(pixel_arrays) < self.max_slices:\n                pixel_arrays.append(pixel_arrays[-1])\n        else:\n            pixel_arrays = pixel_arrays[:self.max_slices]\n        \n        volume = np.stack(pixel_arrays, axis=0).astype(np.float32)\n        \n        # Light smoothing\n        volume = ndimage.gaussian_filter(volume, sigma=0.3)\n        \n        return volume\n    \n    def _get_dummy_volume(self):\n        self.stats['dummy'] += 1\n        volume = np.random.normal(0.3, 0.1, self.target_size).astype(np.float32)\n        return np.clip(volume, 0, 1)\n    \n    def clear_cache(self):\n        self.cache.clear()\n        gc.collect()\n\nclass LightweightAugmentation:\n    def __init__(self, prob=0.2):\n        self.prob = prob\n        self.transform = A.Compose([\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n            A.GaussNoise(var_limit=(5.0, 25.0), p=0.2),\n        ], p=self.prob)\n    \n    def __call__(self, volume):\n        if np.random.random() > self.prob:\n            return volume\n        \n        # Apply to random slices only\n        n_slices_to_augment = max(1, volume.shape[0] // 3)\n        slice_indices = np.random.choice(volume.shape[0], n_slices_to_augment, replace=False)\n        \n        for i in slice_indices:\n            slice_2d = volume[i]\n            slice_uint8 = (slice_2d * 255).astype(np.uint8)\n            transformed = self.transform(image=slice_uint8)\n            volume[i] = transformed['image'].astype(np.float32) / 255.0\n        \n        return volume\n\nclass OptimizedAneurysmDataset(Dataset):\n    def __init__(self, df, series_dir, processor, augmentation=None, mode='train'):\n        self.df = df.copy().reset_index(drop=True)\n        self.series_dir = series_dir\n        self.processor = processor\n        self.augmentation = augmentation\n        self.mode = mode\n        \n        # Pre-validate series paths\n        self._validate_series_paths()\n        \n        logger.info(f\"Dataset created with {len(self.df)} samples\")\n        logger.info(f\"Positive cases: {self.df[Config.TARGET_COL].sum()}\")\n        \n    def _validate_series_paths(self):\n        valid_indices = []\n        for idx, row in self.df.iterrows():\n            series_id = str(row[Config.ID_COL])\n            series_path = os.path.join(self.series_dir, series_id)\n            if os.path.exists(series_path):\n                valid_indices.append(idx)\n            elif len(valid_indices) < len(self.df) * 0.5:\n                valid_indices.append(idx)\n        \n        if len(valid_indices) < len(self.df):\n            logger.warning(f\"Only {len(valid_indices)}/{len(self.df)} series paths exist\")\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        try:\n            row = self.df.iloc[idx]\n            series_id = str(row[Config.ID_COL])\n            label = float(row[Config.TARGET_COL])\n            \n            series_path = os.path.join(self.series_dir, series_id)\n            volume = self.processor.load_dicom_series(series_path)\n            \n            # Apply augmentation\n            if self.augmentation and self.mode == 'train':\n                volume = self.augmentation(volume)\n            \n            # Convert to tensor\n            volume_tensor = torch.from_numpy(volume).float().unsqueeze(0)\n            label_tensor = torch.tensor(label, dtype=torch.float32)\n            \n            return {\n                'volume': volume_tensor,\n                'label': label_tensor,\n                'series_id': series_id,\n                'idx': idx\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error loading sample {idx}: {e}\")\n            return {\n                'volume': torch.zeros((1, *Config.TARGET_SIZE), dtype=torch.float32),\n                'label': torch.tensor(0.0, dtype=torch.float32),\n                'series_id': f\"DUMMY_{idx}\",\n                'idx': idx\n            }\n\nclass EfficientAneurysmNet(nn.Module):\n    def __init__(self, in_channels=1, num_classes=1, dropout_rate=0.3):\n        super(EfficientAneurysmNet, self).__init__()\n        \n        # Efficient backbone\n        self.features = nn.Sequential(\n            # Initial block\n            nn.Conv3d(in_channels, 32, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm3d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 1\n            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool3d(kernel_size=2, stride=2),\n            \n            # Block 2\n            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool3d((1, 1, 1)),\n        )\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, 64),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate * 0.5),\n            nn.Linear(64, num_classes)\n        )\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, pos_weight=None):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.pos_weight = pos_weight\n    \n    def forward(self, inputs, targets):\n        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n            inputs.view(-1), targets, pos_weight=self.pos_weight, reduction='none'\n        )\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n        return focal_loss.mean()\n\ndef create_optimized_data_loaders(train_df, val_df, processor):\n    augmentation = LightweightAugmentation(prob=Config.AUGMENTATION_PROB) if Config.USE_AUGMENTATION else None\n    \n    train_dataset = OptimizedAneurysmDataset(\n        train_df, Config.SERIES_DIR, processor, augmentation, mode='train'\n    )\n    val_dataset = OptimizedAneurysmDataset(\n        val_df, Config.SERIES_DIR, processor, mode='val'\n    )\n    \n    # Balanced sampler\n    targets = [train_dataset[i]['label'].item() for i in range(len(train_dataset))]\n    class_counts = Counter(targets)\n    weights = [len(targets) / (len(class_counts) * class_counts[target]) for target in targets]\n    sampler = WeightedRandomSampler(weights, len(weights))\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=Config.BATCH_SIZE,\n        sampler=sampler,\n        num_workers=2,\n        pin_memory=True if torch.cuda.is_available() and not TPU_AVAILABLE else False,\n        persistent_workers=False,\n        prefetch_factor=Config.PREFETCH_FACTOR\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=Config.BATCH_SIZE,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True if torch.cuda.is_available() and not TPU_AVAILABLE else False,\n        persistent_workers=False\n    )\n    \n    return train_loader, val_loader\n\ndef train_epoch_optimized(model, loader, optimizer, criterion, device, epoch):\n    model.train()\n    running_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n    \n    for batch_idx, batch in enumerate(progress_bar):\n        try:\n            volume = batch['volume'].to(device, non_blocking=True)\n            label = batch['label'].to(device, non_blocking=True)\n            \n            optimizer.zero_grad()\n            outputs = model(volume)\n            loss = criterion(outputs, label)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), Config.GRADIENT_CLIP)\n            \n            if TPU_AVAILABLE:\n                xm.optimizer_step(optimizer)\n            else:\n                optimizer.step()\n            \n            # Metrics\n            running_loss += loss.item()\n            with torch.no_grad():\n                probs = torch.sigmoid(outputs).cpu().numpy()\n                preds = (probs > 0.5).astype(int)\n                all_preds.extend(preds.flatten())\n                all_labels.extend(label.cpu().numpy())\n            \n            # Update progress\n            if batch_idx % 10 == 0:\n                progress_bar.set_postfix({\n                    'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                    'Acc': f'{np.mean(np.array(all_preds) == np.array(all_labels)):.3f}'\n                })\n            \n            # Memory cleanup\n            if batch_idx % 20 == 0 and torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                \n        except Exception as e:\n            logger.error(f\"Error in batch {batch_idx}: {e}\")\n            continue\n    \n    # Calculate metrics\n    metrics = {\n        'loss': running_loss / len(loader),\n        'accuracy': accuracy_score(all_labels, all_preds),\n        'f1': f1_score(all_labels, all_preds, zero_division=0),\n    }\n    \n    if len(np.unique(all_labels)) > 1:\n        probs = torch.sigmoid(torch.tensor(all_preds, dtype=torch.float32)).numpy()\n        metrics['auc'] = roc_auc_score(all_labels, probs)\n    else:\n        metrics['auc'] = 0.5\n    \n    return metrics\n\ndef validate_epoch_optimized(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(tqdm(loader, desc=\"Validation\", leave=False)):\n            try:\n                volume = batch['volume'].to(device, non_blocking=True)\n                label = batch['label'].to(device, non_blocking=True)\n                \n                outputs = model(volume)\n                loss = criterion(outputs, label)\n                \n                running_loss += loss.item()\n                probs = torch.sigmoid(outputs).cpu().numpy()\n                preds = (probs > 0.5).astype(int)\n                \n                all_preds.extend(preds.flatten())\n                all_probs.extend(probs.flatten())\n                all_labels.extend(label.cpu().numpy())\n                \n            except Exception as e:\n                logger.error(f\"Error in validation batch {batch_idx}: {e}\")\n                continue\n    \n    # Calculate metrics\n    metrics = {\n        'loss': running_loss / len(loader),\n        'accuracy': accuracy_score(all_labels, all_preds),\n        'f1': f1_score(all_labels, all_preds, zero_division=0),\n    }\n    \n    if len(np.unique(all_labels)) > 1:\n        metrics['auc'] = roc_auc_score(all_labels, all_probs)\n    else:\n        metrics['auc'] = 0.5\n    \n    return metrics\n\ndef optimized_training_pipeline():\n    print(\"OPTIMIZED ANEURYSM DETECTION TRAINING\")\n    print(\"=\" * 60)\n    print(f\"Device: {Config.DEVICE}\")\n    print(f\"Debug Mode: {Config.DEBUG_MODE}\")\n    print(f\"Target Size: {Config.TARGET_SIZE}\")\n    print(f\"Batch Size: {Config.BATCH_SIZE}\")\n    \n    # Load data\n    try:\n        train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n        print(f\"Loaded {len(train_df)} samples\")\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        return None\n    \n    # Data validation\n    train_df = train_df.dropna(subset=[Config.ID_COL, Config.TARGET_COL])\n    train_df = train_df[train_df[Config.TARGET_COL].isin([0, 1])]\n    \n    if Config.DEBUG_MODE:\n        # Sample balanced data for debugging\n        pos_samples = train_df[train_df[Config.TARGET_COL] == 1].sample(\n            n=min(Config.DEBUG_SAMPLES//2, train_df[Config.TARGET_COL].sum()), \n            random_state=42\n        )\n        neg_samples = train_df[train_df[Config.TARGET_COL] == 0].sample(\n            n=min(Config.DEBUG_SAMPLES//2, (train_df[Config.TARGET_COL] == 0).sum()), \n            random_state=42\n        )\n        train_df = pd.concat([pos_samples, neg_samples]).reset_index(drop=True)\n        print(f\"Debug mode: using {len(train_df)} samples\")\n    \n    print(f\"Class distribution: {train_df[Config.TARGET_COL].value_counts().sort_index().to_dict()}\")\n    \n    # Cross-validation setup\n    skf = StratifiedKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=42)\n    fold_results = []\n    \n    processor = OptimizedDICOMProcessor()\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df[Config.TARGET_COL])):\n        if Config.CURRENT_FOLD >= 0 and fold != Config.CURRENT_FOLD:\n            continue\n            \n        print(f\"\\nFOLD {fold + 1}/{Config.N_FOLDS}\")\n        print(\"-\" * 40)\n        \n        fold_train_df = train_df.iloc[train_idx].reset_index(drop=True)\n        fold_val_df = train_df.iloc[val_idx].reset_index(drop=True)\n        \n        print(f\"Train: {len(fold_train_df)}, Val: {len(fold_val_df)}\")\n        print(f\"Train pos: {fold_train_df[Config.TARGET_COL].sum()}, Val pos: {fold_val_df[Config.TARGET_COL].sum()}\")\n        \n        # Data loaders\n        train_loader, val_loader = create_optimized_data_loaders(fold_train_df, fold_val_df, processor)\n        \n        # Model setup\n        model = EfficientAneurysmNet().to(Config.DEVICE)\n        \n        # Loss function with class weights\n        pos_count = fold_train_df[Config.TARGET_COL].sum()\n        neg_count = len(fold_train_df) - pos_count\n        pos_weight = torch.tensor([neg_count / max(pos_count, 1)]).to(Config.DEVICE)\n        \n        criterion = FocalLoss(alpha=1, gamma=2, pos_weight=pos_weight)\n        \n        # Optimizer and scheduler\n        optimizer = optim.AdamW(\n            model.parameters(),\n            lr=Config.LEARNING_RATE,\n            weight_decay=Config.WEIGHT_DECAY\n        )\n        \n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='min', patience=Config.SCHEDULER_PATIENCE,\n            factor=0.5, verbose=True\n        )\n        \n        # Training loop\n        best_val_auc = 0\n        patience_counter = 0\n        history = {'train_loss': [], 'val_loss': [], 'train_auc': [], 'val_auc': []}\n        \n        for epoch in range(Config.EPOCHS):\n            print(f\"\\nEpoch {epoch+1}/{Config.EPOCHS}\")\n            \n            # Training\n            train_metrics = train_epoch_optimized(model, train_loader, optimizer, criterion, Config.DEVICE, epoch)\n            \n            # Validation\n            val_metrics = validate_epoch_optimized(model, val_loader, criterion, Config.DEVICE)\n            \n            # Scheduler step\n            scheduler.step(val_metrics['loss'])\n            \n            # Record history\n            history['train_loss'].append(train_metrics['loss'])\n            history['val_loss'].append(val_metrics['loss'])\n            history['train_auc'].append(train_metrics['auc'])\n            history['val_auc'].append(val_metrics['auc'])\n            \n            # Print metrics\n            print(f\"Train - Loss: {train_metrics['loss']:.4f}, AUC: {train_metrics['auc']:.4f}, Acc: {train_metrics['accuracy']:.4f}\")\n            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, AUC: {val_metrics['auc']:.4f}, Acc: {val_metrics['accuracy']:.4f}\")\n            \n            # Early stopping and model saving\n            if val_metrics['auc'] > best_val_auc:\n                best_val_auc = val_metrics['auc']\n                patience_counter = 0\n                \n                # Save best model\n                torch.save({\n                    'fold': fold,\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_auc': best_val_auc,\n                    'history': history\n                }, f'fold_{fold}_best_model.pth')\n                \n                print(f\"New best AUC: {best_val_auc:.4f} - Model saved!\")\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= Config.EARLY_STOPPING_PATIENCE:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n            \n            # Memory cleanup\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Save fold results\n        fold_results.append({\n            'fold': fold + 1,\n            'best_val_auc': best_val_auc,\n            'history': history\n        })\n        \n        print(f\"Fold {fold + 1} completed! Best AUC: {best_val_auc:.4f}\")\n        \n        # Clear processor cache\n        processor.clear_cache()\n        \n        if Config.CURRENT_FOLD >= 0:\n            break\n    \n    # Summary\n    if fold_results:\n        aucs = [result['best_val_auc'] for result in fold_results]\n        print(f\"\\nCROSS-VALIDATION SUMMARY\")\n        print(\"=\" * 40)\n        print(f\"Mean AUC: {np.mean(aucs):.4f} Â± {np.std(aucs):.4f}\")\n        print(f\"Best AUC: {max(aucs):.4f}\")\n        \n        # Save results\n        with open('optimized_results.json', 'w') as f:\n            json.dump(fold_results, f, indent=2, default=str)\n    \n    return fold_results","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:04:00.290160Z","iopub.execute_input":"2025-09-04T18:04:00.290462Z","iopub.status.idle":"2025-09-04T18:04:00.361487Z","shell.execute_reply.started":"2025-09-04T18:04:00.290439Z","shell.execute_reply":"2025-09-04T18:04:00.356829Z"}},"outputs":[{"name":"stdout","text":"Using TPU device: xla:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    results = optimized_training_pipeline()\n    \n    if results:\n        print(\"\\nTraining completed successfully!\")\n        print(\"Check optimized_results.json for detailed results\")\n    else:\n        print(\"\\nTraining failed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model Evaluation and Performance**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n    roc_curve, precision_recall_curve, confusion_matrix, classification_report,\n    average_precision_score, balanced_accuracy_score, matthews_corrcoef,\n    cohen_kappa_score, log_loss, brier_score_loss\n)\nfrom sklearn.calibration import calibration_curve\nfrom scipy import stats\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nfrom tqdm.auto import tqdm\nimport warnings\nimport json\nfrom datetime import datetime\nfrom collections import defaultdict\nimport itertools\nfrom scipy.stats import bootstrap\nwarnings.filterwarnings('ignore')\n\nclass AdvancedModelEvaluator:\n    def __init__(self, model_paths, data_loader, device, class_names=None, save_dir=\"evaluation_results\"):\n        self.model_paths = model_paths if isinstance(model_paths, list) else [model_paths]\n        self.data_loader = data_loader\n        self.device = device\n        self.class_names = class_names or ['No Aneurysm', 'Aneurysm']\n        self.save_dir = save_dir\n        \n        # Create save directory\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # Storage for results\n        self.results = {}\n        self.predictions = {}\n        self.raw_outputs = {}\n        \n    def load_model_predictions(self, model_class):\n        \"\"\"Load models and generate predictions\"\"\"\n        print(\"Loading models and generating predictions...\")\n        \n        for i, model_path in enumerate(self.model_paths):\n            print(f\"\\nProcessing model: {model_path}\")\n            \n            # Load model\n            model = model_class().to(self.device)\n            \n            if os.path.exists(model_path):\n                checkpoint = torch.load(model_path, map_location=self.device)\n                model.load_state_dict(checkpoint['model_state_dict'])\n                print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n            else:\n                print(f\"Model file not found: {model_path}\")\n                continue\n            \n            # Generate predictions\n            model.eval()\n            predictions = []\n            probabilities = []\n            labels = []\n            logits = []\n            series_ids = []\n            \n            with torch.no_grad():\n                for batch in tqdm(self.data_loader, desc=f\"Model {i+1} Inference\"):\n                    volume = batch['volume'].to(self.device)\n                    label = batch['label']\n                    series_id = batch.get('series_id', [f'sample_{j}' for j in range(len(label))])\n                    \n                    outputs = model(volume)\n                    probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n                    preds = (probs > 0.5).astype(int)\n                    \n                    predictions.extend(preds)\n                    probabilities.extend(probs)\n                    labels.extend(label.numpy())\n                    logits.extend(outputs.cpu().numpy().flatten())\n                    series_ids.extend(series_id)\n            \n            model_name = f\"Model_{i+1}\" if len(self.model_paths) > 1 else \"Model\"\n            \n            self.predictions[model_name] = {\n                'predictions': np.array(predictions),\n                'probabilities': np.array(probabilities),\n                'labels': np.array(labels),\n                'logits': np.array(logits),\n                'series_ids': series_ids,\n                'model_path': model_path\n            }\n    \n    def calculate_comprehensive_metrics(self):\n        print(\"\\nCalculating comprehensive metrics...\")\n        \n        for model_name, data in self.predictions.items():\n            y_true = data['labels']\n            y_pred = data['predictions']\n            y_proba = data['probabilities']\n            y_logits = data['logits']\n            \n            # Basic classification metrics\n            basic_metrics = {\n                'accuracy': accuracy_score(y_true, y_pred),\n                'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n                'precision': precision_score(y_true, y_pred, zero_division=0),\n                'recall': recall_score(y_true, y_pred, zero_division=0),\n                'specificity': recall_score(1 - y_true, 1 - y_pred, zero_division=0),\n                'f1_score': f1_score(y_true, y_pred, zero_division=0),\n                'matthews_corr': matthews_corrcoef(y_true, y_pred),\n                'cohen_kappa': cohen_kappa_score(y_true, y_pred),\n            }\n            \n            # Probabilistic metrics\n            if len(np.unique(y_true)) > 1:\n                prob_metrics = {\n                    'roc_auc': roc_auc_score(y_true, y_proba),\n                    'pr_auc': average_precision_score(y_true, y_proba),\n                    'log_loss': log_loss(y_true, y_proba),\n                    'brier_score': brier_score_loss(y_true, y_proba),\n                }\n            else:\n                prob_metrics = {\n                    'roc_auc': 0.5,\n                    'pr_auc': np.mean(y_true),\n                    'log_loss': float('inf'),\n                    'brier_score': float('inf'),\n                }\n            \n            # Threshold-dependent metrics\n            threshold_metrics = self._calculate_threshold_metrics(y_true, y_proba)\n            \n            # Confidence and calibration metrics\n            calibration_metrics = self._calculate_calibration_metrics(y_true, y_proba)\n            \n            # Class-wise metrics\n            class_metrics = self._calculate_class_wise_metrics(y_true, y_pred, y_proba)\n            \n            # Ensemble metrics (if multiple models)\n            ensemble_metrics = {}\n            if len(self.predictions) > 1:\n                ensemble_metrics = self._calculate_ensemble_metrics()\n            \n            # Combine all metrics\n            all_metrics = {\n                **basic_metrics,\n                **prob_metrics,\n                **threshold_metrics,\n                **calibration_metrics,\n                **class_metrics,\n                **ensemble_metrics\n            }\n            \n            self.results[model_name] = all_metrics\n    \n    def _calculate_threshold_metrics(self, y_true, y_proba):\n        thresholds = np.arange(0.1, 1.0, 0.1)\n        threshold_results = {}\n        \n        for threshold in thresholds:\n            y_pred_thresh = (y_proba >= threshold).astype(int)\n            \n            if len(np.unique(y_pred_thresh)) > 1:\n                threshold_results[f'f1_thresh_{threshold:.1f}'] = f1_score(y_true, y_pred_thresh)\n                threshold_results[f'precision_thresh_{threshold:.1f}'] = precision_score(y_true, y_pred_thresh, zero_division=0)\n                threshold_results[f'recall_thresh_{threshold:.1f}'] = recall_score(y_true, y_pred_thresh, zero_division=0)\n        \n        # Find optimal threshold\n        if len(np.unique(y_true)) > 1:\n            fpr, tpr, thresholds_roc = roc_curve(y_true, y_proba)\n            optimal_idx = np.argmax(tpr - fpr)\n            optimal_threshold = thresholds_roc[optimal_idx]\n            \n            threshold_results['optimal_threshold'] = optimal_threshold\n            threshold_results['optimal_f1'] = f1_score(y_true, (y_proba >= optimal_threshold).astype(int))\n        \n        return threshold_results\n    \n    def _calculate_calibration_metrics(self, y_true, y_proba):\n        # Calibration curve\n        if len(np.unique(y_true)) > 1:\n            prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n            calibration_error = np.mean(np.abs(prob_true - prob_pred))\n            \n            # Expected Calibration Error (ECE)\n            bin_boundaries = np.linspace(0, 1, 11)\n            bin_lowers = bin_boundaries[:-1]\n            bin_uppers = bin_boundaries[1:]\n            \n            ece = 0\n            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n                in_bin = (y_proba > bin_lower) & (y_proba <= bin_upper)\n                prop_in_bin = in_bin.mean()\n                \n                if prop_in_bin > 0:\n                    accuracy_in_bin = y_true[in_bin].mean()\n                    avg_confidence_in_bin = y_proba[in_bin].mean()\n                    ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n        else:\n            calibration_error = float('inf')\n            ece = float('inf')\n        \n        # Confidence metrics\n        confidence_metrics = {\n            'mean_confidence': np.mean(y_proba),\n            'std_confidence': np.std(y_proba),\n            'confidence_range': np.ptp(y_proba),\n            'calibration_error': calibration_error,\n            'expected_calibration_error': ece,\n        }\n        \n        return confidence_metrics\n    \n    def _calculate_class_wise_metrics(self, y_true, y_pred, y_proba):\n        cm = confusion_matrix(y_true, y_pred)\n        \n        if cm.shape == (2, 2):\n            tn, fp, fn, tp = cm.ravel()\n            \n            class_metrics = {\n                'true_positives': tp,\n                'true_negatives': tn,\n                'false_positives': fp,\n                'false_negatives': fn,\n                'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,\n                'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n                'positive_predictive_value': tp / (tp + fp) if (tp + fp) > 0 else 0,\n                'negative_predictive_value': tn / (tn + fn) if (tn + fn) > 0 else 0,\n                'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0,\n                'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0,\n                'positive_likelihood_ratio': (tp/(tp+fn)) / (fp/(fp+tn)) if fp > 0 else float('inf'),\n                'negative_likelihood_ratio': (fn/(fn+tp)) / (tn/(tn+fp)) if tn > 0 else float('inf'),\n            }\n        else:\n            class_metrics = {}\n        \n        return class_metrics\n    \n    def _calculate_ensemble_metrics(self):\n        if len(self.predictions) < 2:\n            return {}\n        \n        # Average predictions across models\n        all_probas = []\n        all_preds = []\n        y_true = None\n        \n        for model_name, data in self.predictions.items():\n            all_probas.append(data['probabilities'])\n            all_preds.append(data['predictions'])\n            if y_true is None:\n                y_true = data['labels']\n        \n        # Ensemble by averaging probabilities\n        ensemble_proba = np.mean(all_probas, axis=0)\n        ensemble_pred = (ensemble_proba > 0.5).astype(int)\n        \n        # Ensemble by majority voting\n        majority_pred = np.round(np.mean(all_preds, axis=0)).astype(int)\n        \n        ensemble_metrics = {\n            'ensemble_accuracy': accuracy_score(y_true, ensemble_pred),\n            'ensemble_f1': f1_score(y_true, ensemble_pred),\n            'ensemble_auc': roc_auc_score(y_true, ensemble_proba) if len(np.unique(y_true)) > 1 else 0.5,\n            'majority_vote_accuracy': accuracy_score(y_true, majority_pred),\n            'majority_vote_f1': f1_score(y_true, majority_pred),\n        }\n        \n        # Model agreement\n        agreement_matrix = np.array(all_preds)\n        model_agreement = np.mean(np.std(agreement_matrix, axis=0) == 0)  # Fraction of samples where all models agree\n        \n        ensemble_metrics['model_agreement'] = model_agreement\n        \n        return ensemble_metrics\n    \n    def create_comprehensive_visualizations(self):\n        print(\"\\nCreating comprehensive visualizations...\")\n        \n        # Set style\n        plt.style.use('seaborn-v0_8')\n        colors = ['#2E8B57', '#DC143C', '#4169E1', '#FFD700', '#8A2BE2']\n        \n        # 1. Performance Overview Dashboard\n        self._create_performance_dashboard()\n        \n        # 2. ROC Curves Comparison\n        self._create_roc_comparison()\n        \n        # 3. Precision-Recall Curves\n        self._create_pr_comparison()\n        \n        # 4. Calibration Plots\n        self._create_calibration_plots()\n        \n        # 5. Confusion Matrices\n        self._create_confusion_matrices()\n        \n        # 6. Threshold Analysis\n        self._create_threshold_analysis()\n        \n        # 7. Prediction Distribution Analysis\n        self._create_prediction_distributions()\n        \n        # 8. Error Analysis\n        self._create_error_analysis()\n        \n        # 9. Interactive Plotly Visualizations\n        self._create_interactive_plots()\n        \n        # 10. Statistical Significance Tests\n        self._create_statistical_tests()\n    \n    def _create_performance_dashboard(self):\n        n_models = len(self.predictions)\n        fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n        fig.suptitle('Model Performance Dashboard', fontsize=16, fontweight='bold')\n        \n        # Key metrics comparison\n        metrics_to_plot = ['accuracy', 'f1_score', 'roc_auc', 'pr_auc', \n                          'precision', 'recall', 'specificity', 'matthews_corr']\n        \n        for i, metric in enumerate(metrics_to_plot):\n            ax = axes[i // 4, i % 4]\n            \n            values = [self.results[model][metric] for model in self.results.keys()]\n            model_names = list(self.results.keys())\n            \n            bars = ax.bar(model_names, values, color=colors[:len(values)])\n            ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontweight='bold')\n            ax.set_ylabel('Score')\n            ax.set_ylim(0, 1)\n            \n            # Add value labels on bars\n            for bar, value in zip(bars, values):\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                       f'{value:.3f}', ha='center', va='bottom')\n            \n            ax.tick_params(axis='x', rotation=45)\n        \n        # Remove empty subplots\n        for j in range(len(metrics_to_plot), 12):\n            axes[j // 4, j % 4].remove()\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/performance_dashboard.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_roc_comparison(self):\n        plt.figure(figsize=(10, 8))\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                fpr, tpr, _ = roc_curve(y_true, y_proba)\n                auc = roc_auc_score(y_true, y_proba)\n                \n                plt.plot(fpr, tpr, color=colors[i], lw=2,\n                        label=f'{model_name} (AUC = {auc:.3f})')\n        \n        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.8)\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate', fontsize=12)\n        plt.ylabel('True Positive Rate', fontsize=12)\n        plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n        plt.legend(loc=\"lower right\")\n        plt.grid(True, alpha=0.3)\n        \n        plt.savefig(f'{self.save_dir}/roc_comparison.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_pr_comparison(self):\n        plt.figure(figsize=(10, 8))\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                precision, recall, _ = precision_recall_curve(y_true, y_proba)\n                avg_precision = average_precision_score(y_true, y_proba)\n                \n                plt.plot(recall, precision, color=colors[i], lw=2,\n                        label=f'{model_name} (AP = {avg_precision:.3f})')\n        \n        # Baseline\n        baseline = np.mean([data['labels'] for data in self.predictions.values()][0])\n        plt.axhline(y=baseline, color='gray', linestyle='--', alpha=0.8, \n                   label=f'Baseline (AP = {baseline:.3f})')\n        \n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('Recall', fontsize=12)\n        plt.ylabel('Precision', fontsize=12)\n        plt.title('Precision-Recall Curve Comparison', fontsize=14, fontweight='bold')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.savefig(f'{self.save_dir}/pr_comparison.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_calibration_plots(self):\n        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Calibration curve\n        ax1 = axes[0]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n                ax1.plot(prob_pred, prob_true, marker='o', color=colors[i], \n                        label=model_name, linewidth=2, markersize=6)\n        \n        ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n        ax1.set_xlabel('Mean Predicted Probability')\n        ax1.set_ylabel('Fraction of Positives')\n        ax1.set_title('Calibration Plot (Reliability Diagram)')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Confidence histogram\n        ax2 = axes[1]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_proba = data['probabilities']\n            ax2.hist(y_proba, bins=20, alpha=0.7, color=colors[i], \n                    label=model_name, density=True)\n        \n        ax2.set_xlabel('Predicted Probability')\n        ax2.set_ylabel('Density')\n        ax2.set_title('Confidence Distribution')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/calibration_plots.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_confusion_matrices(self):\n        n_models = len(self.predictions)\n        cols = min(3, n_models)\n        rows = (n_models + cols - 1) // cols\n        \n        fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n        if n_models == 1:\n            axes = [axes]\n        elif rows == 1:\n            axes = [axes]\n        else:\n            axes = axes.flatten()\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_pred = data['predictions']\n            \n            cm = confusion_matrix(y_true, y_pred)\n            \n            # Normalize confusion matrix\n            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            \n            ax = axes[i] if n_models > 1 else axes[0]\n            \n            # Create heatmap\n            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n                       xticklabels=self.class_names, yticklabels=self.class_names,\n                       ax=ax, cbar_kws={'shrink': 0.8})\n            \n            ax.set_title(f'{model_name}\\nNormalized Confusion Matrix')\n            ax.set_ylabel('True Label')\n            ax.set_xlabel('Predicted Label')\n            \n            # Add counts\n            for j in range(cm.shape[0]):\n                for k in range(cm.shape[1]):\n                    ax.text(k+0.5, j+0.7, f'({cm[j,k]})', \n                           ha='center', va='center', fontsize=10, color='red')\n        \n        # Remove empty subplots\n        for j in range(n_models, len(axes)):\n            if j < len(axes):\n                fig.delaxes(axes[j])\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_threshold_analysis(self):\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        \n        # Threshold vs F1 Score\n        ax1 = axes[0, 0]\n        thresholds = np.arange(0.1, 1.0, 0.01)\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            f1_scores = []\n            for threshold in thresholds:\n                y_pred_thresh = (y_proba >= threshold).astype(int)\n                f1_scores.append(f1_score(y_true, y_pred_thresh))\n            \n            ax1.plot(thresholds, f1_scores, color=colors[i], label=model_name, linewidth=2)\n            \n            # Mark optimal threshold\n            optimal_idx = np.argmax(f1_scores)\n            ax1.scatter(thresholds[optimal_idx], f1_scores[optimal_idx], \n                       color=colors[i], s=100, marker='*', zorder=5)\n        \n        ax1.set_xlabel('Threshold')\n        ax1.set_ylabel('F1 Score')\n        ax1.set_title('F1 Score vs Threshold')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Threshold vs Precision/Recall\n        ax2 = axes[0, 1]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            precisions = []\n            recalls = []\n            for threshold in thresholds:\n                y_pred_thresh = (y_proba >= threshold).astype(int)\n                precisions.append(precision_score(y_true, y_pred_thresh, zero_division=0))\n                recalls.append(recall_score(y_true, y_pred_thresh, zero_division=0))\n            \n            ax2.plot(thresholds, precisions, color=colors[i], linestyle='-', \n                    label=f'{model_name} Precision', linewidth=2)\n            ax2.plot(thresholds, recalls, color=colors[i], linestyle='--', \n                    label=f'{model_name} Recall', linewidth=2)\n        \n        ax2.set_xlabel('Threshold')\n        ax2.set_ylabel('Score')\n        ax2.set_title('Precision/Recall vs Threshold')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        # Class distribution by confidence\n        ax3 = axes[1, 0]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            # Separate by true class\n            pos_probs = y_proba[y_true == 1]\n            neg_probs = y_proba[y_true == 0]\n            \n            ax3.hist(neg_probs, bins=20, alpha=0.5, color=colors[i], \n                    label=f'{model_name} Negative', density=True)\n            ax3.hist(pos_probs, bins=20, alpha=0.5, color=colors[i], \n                    label=f'{model_name} Positive', density=True, hatch='///')\n        \n        ax3.set_xlabel('Predicted Probability')\n        ax3.set_ylabel('Density')\n        ax3.set_title('Probability Distribution by True Class')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n        \n        # Sensitivity/Specificity vs Threshold\n        ax4 = axes[1, 1]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            sensitivities = []\n            specificities = []\n            for threshold in thresholds:\n                y_pred_thresh = (y_proba >= threshold).astype(int)\n                sensitivities.append(recall_score(y_true, y_pred_thresh, zero_division=0))\n                specificities.append(recall_score(1 - y_true, 1 - y_pred_thresh, zero_division=0))\n            \n            ax4.plot(thresholds, sensitivities, color=colors[i], linestyle='-', \n                    label=f'{model_name} Sensitivity', linewidth=2)\n            ax4.plot(thresholds, specificities, color=colors[i], linestyle='--', \n                    label=f'{model_name} Specificity', linewidth=2)\n        \n        ax4.set_xlabel('Threshold')\n        ax4.set_ylabel('Score')\n        ax4.set_title('Sensitivity/Specificity vs Threshold')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/threshold_analysis.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_prediction_distributions(self):\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        \n        # Box plots of predictions by true class\n        ax1 = axes[0, 0]\n        box_data = []\n        box_labels = []\n        \n        for model_name, data in self.predictions.items():\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            for class_idx, class_name in enumerate(self.class_names):\n                class_probs = y_proba[y_true == class_idx]\n                box_data.append(class_probs)\n                box_labels.append(f'{model_name}\\n{class_name}')\n        \n        ax1.boxplot(box_data, labels=box_labels)\n        ax1.set_title('Prediction Distribution by Model and True Class')\n        ax1.set_ylabel('Predicted Probability')\n        ax1.tick_params(axis='x', rotation=45)\n        ax1.grid(True, alpha=0.3)\n        \n        # Violin plots\n        ax2 = axes[0, 1]\n        positions = np.arange(1, len(box_data) + 1)\n        parts = ax2.violinplot(box_data, positions=positions, showmeans=True, showmedians=True)\n        ax2.set_xticks(positions)\n        ax2.set_xticklabels(box_labels, rotation=45)\n        ax2.set_title('Prediction Distribution (Violin Plot)')\n        ax2.set_ylabel('Predicted Probability')\n        ax2.grid(True, alpha=0.3)\n        \n        # Prediction confidence vs accuracy\n        ax3 = axes[1, 0]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_pred = data['predictions']\n            y_proba = data['probabilities']\n            \n            # Bin predictions by confidence\n            confidence_bins = np.linspace(0, 1, 11)\n            bin_accuracies = []\n            bin_centers = []\n            \n            for j in range(len(confidence_bins) - 1):\n                mask = (y_proba >= confidence_bins[j]) & (y_proba < confidence_bins[j + 1])\n                if mask.sum() > 0:\n                    bin_accuracy = (y_pred[mask] == y_true[mask]).mean()\n                    bin_accuracies.append(bin_accuracy)\n                    bin_centers.append((confidence_bins[j] + confidence_bins[j + 1]) / 2)\n            \n            ax3.plot(bin_centers, bin_accuracies, 'o-', color=colors[i], \n                    label=model_name, linewidth=2, markersize=6)\n        \n        ax3.set_xlabel('Prediction Confidence (Binned)')\n        ax3.set_ylabel('Accuracy')\n        ax3.set_title('Confidence vs Accuracy')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n        \n        # Error distribution\n        ax4 = axes[1, 1]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_pred = data['predictions']\n            \n            # Calculate error types\n            tp = ((y_true == 1) & (y_pred == 1)).sum()\n            tn = ((y_true == 0) & (y_pred == 0)).sum()\n            fp = ((y_true == 0) & (y_pred == 1)).sum()\n            fn = ((y_true == 1) & (y_pred == 0)).sum()\n            \n            categories = ['True Pos', 'True Neg', 'False Pos', 'False Neg']\n            values = [tp, tn, fp, fn]\n            \n            x_pos = np.arange(len(categories)) + i * 0.2\n            ax4.bar(x_pos, values, width=0.2, color=colors[i], \n                   label=model_name, alpha=0.8)\n        \n        ax4.set_xlabel('Prediction Type')\n        ax4.set_ylabel('Count')\n        ax4.set_title('Error Type Distribution')\n        ax4.set_xticks(np.arange(len(categories)) + 0.1)\n        ax4.set_xticklabels(categories)\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/prediction_distributions.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_error_analysis(self):\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            if i >= 6:  # Limit to 6 models for visualization\n                break\n                \n            y_true = data['labels']\n            y_pred = data['predictions']\n            y_proba = data['probabilities']\n            \n            ax = axes[i // 3, i % 3]\n            \n            # Create error analysis scatter plot\n            correct = (y_pred == y_true)\n            \n            # Plot correct predictions\n            ax.scatter(y_proba[correct], y_true[correct], \n                      alpha=0.6, c='green', label='Correct', s=20)\n            \n            # Plot incorrect predictions\n            ax.scatter(y_proba[~correct], y_true[~correct], \n                      alpha=0.6, c='red', label='Incorrect', s=20, marker='x')\n            \n            ax.set_xlabel('Predicted Probability')\n            ax.set_ylabel('True Label')\n            ax.set_title(f'{model_name} - Error Analysis')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n            \n            # Add decision boundary\n            ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n        \n        # Remove empty subplots\n        for j in range(len(self.predictions), 6):\n            axes[j // 3, j % 3].remove()\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/error_analysis.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_interactive_plots(self):\n        print(\"Creating interactive visualizations...\")\n        \n        # Interactive ROC curve\n        fig_roc = go.Figure()\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n                auc = roc_auc_score(y_true, y_proba)\n                \n                fig_roc.add_trace(go.Scatter(\n                    x=fpr, y=tpr,\n                    mode='lines',\n                    name=f'{model_name} (AUC = {auc:.3f})',\n                    line=dict(width=3),\n                    hovertemplate='FPR: %{x:.3f}<br>TPR: %{y:.3f}<extra></extra>'\n                ))\n        \n        # Add diagonal line\n        fig_roc.add_trace(go.Scatter(\n            x=[0, 1], y=[0, 1],\n            mode='lines',\n            name='Random Classifier',\n            line=dict(dash='dash', color='gray', width=2)\n        ))\n        \n        fig_roc.update_layout(\n            title='Interactive ROC Curve Comparison',\n            xaxis_title='False Positive Rate',\n            yaxis_title='True Positive Rate',\n            width=800, height=600,\n            hovermode='closest'\n        )\n        \n        fig_roc.write_html(f'{self.save_dir}/interactive_roc.html')\n        \n        # Interactive threshold analysis\n        fig_threshold = make_subplots(\n            rows=2, cols=2,\n            subplot_titles=('F1 Score vs Threshold', 'Precision/Recall vs Threshold',\n                          'Sensitivity/Specificity vs Threshold', 'Sample Count vs Threshold')\n        )\n        \n        thresholds = np.arange(0.1, 1.0, 0.01)\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            # Calculate metrics for each threshold\n            f1_scores = []\n            precisions = []\n            recalls = []\n            specificities = []\n            sample_counts = []\n            \n            for threshold in thresholds:\n                y_pred_thresh = (y_proba >= threshold).astype(int)\n                f1_scores.append(f1_score(y_true, y_pred_thresh))\n                precisions.append(precision_score(y_true, y_pred_thresh, zero_division=0))\n                recalls.append(recall_score(y_true, y_pred_thresh, zero_division=0))\n                specificities.append(recall_score(1 - y_true, 1 - y_pred_thresh, zero_division=0))\n                sample_counts.append(y_pred_thresh.sum())\n            \n            # F1 Score\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=f1_scores, name=f'{model_name} F1',\n                          line=dict(width=2)),\n                row=1, col=1\n            )\n            \n            # Precision/Recall\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=precisions, name=f'{model_name} Precision',\n                          line=dict(width=2)),\n                row=1, col=2\n            )\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=recalls, name=f'{model_name} Recall',\n                          line=dict(width=2, dash='dash')),\n                row=1, col=2\n            )\n            \n            # Sensitivity\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=recalls, name=f'{model_name} Sensitivity',\n                          line=dict(width=2)),\n                row=2, col=1\n            )\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=specificities, name=f'{model_name} Specificity',\n                          line=dict(width=2, dash='dash')),\n                row=2, col=1\n            )\n            \n            # Sample count\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=sample_counts, name=f'{model_name} Positive Predictions',\n                          line=dict(width=2)),\n                row=2, col=2\n            )\n        \n        fig_threshold.update_layout(\n            title='Interactive Threshold Analysis',\n            width=1200, height=800,\n            hovermode='x unified'\n        )\n        \n        fig_threshold.write_html(f'{self.save_dir}/interactive_threshold.html')\n        \n        # Interactive calibration plot\n        fig_cal = go.Figure()\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n                \n                fig_cal.add_trace(go.Scatter(\n                    x=prob_pred, y=prob_true,\n                    mode='markers+lines',\n                    name=model_name,\n                    marker=dict(size=8),\n                    line=dict(width=3),\n                    hovertemplate='Predicted: %{x:.3f}<br>Actual: %{y:.3f}<extra></extra>'\n                ))\n        \n        # Perfect calibration line\n        fig_cal.add_trace(go.Scatter(\n            x=[0, 1], y=[0, 1],\n            mode='lines',\n            name='Perfect Calibration',\n            line=dict(dash='dash', color='gray', width=2)\n        ))\n        \n        fig_cal.update_layout(\n            title='Interactive Calibration Plot',\n            xaxis_title='Mean Predicted Probability',\n            yaxis_title='Fraction of Positives',\n            width=800, height=600\n        )\n        \n        fig_cal.write_html(f'{self.save_dir}/interactive_calibration.html')\n    \n    def _create_statistical_tests(self):\n        print(\"Performing statistical significance tests...\")\n        \n        if len(self.predictions) < 2:\n            print(\"Need at least 2 models for statistical comparison\")\n            return\n        \n        # McNemar's test for paired predictions\n        results_text = \"STATISTICAL SIGNIFICANCE TESTS\\n\" + \"=\"*50 + \"\\n\\n\"\n        \n        model_names = list(self.predictions.keys())\n        n_models = len(model_names)\n        \n        # Create comparison matrix\n        comparison_results = pd.DataFrame(index=model_names, columns=model_names)\n        \n        for i in range(n_models):\n            for j in range(i+1, n_models):\n                model1_name = model_names[i]\n                model2_name = model_names[j]\n                \n                y_true = self.predictions[model1_name]['labels']\n                pred1 = self.predictions[model1_name]['predictions']\n                pred2 = self.predictions[model2_name]['predictions']\n                \n                # McNemar's test\n                # Create contingency table\n                correct1 = (pred1 == y_true)\n                correct2 = (pred2 == y_true)\n                \n                both_correct = np.sum(correct1 & correct2)\n                model1_only = np.sum(correct1 & ~correct2)\n                model2_only = np.sum(~correct1 & correct2)\n                both_wrong = np.sum(~correct1 & ~correct2)\n                \n                # McNemar's test statistic\n                if (model1_only + model2_only) > 0:\n                    mcnemar_stat = (abs(model1_only - model2_only) - 1)**2 / (model1_only + model2_only)\n                    p_value = 1 - stats.chi2.cdf(mcnemar_stat, 1)\n                else:\n                    mcnemar_stat = 0\n                    p_value = 1.0\n                \n                comparison_results.loc[model1_name, model2_name] = f\"p={p_value:.4f}\"\n                comparison_results.loc[model2_name, model1_name] = f\"ÏÂ²={mcnemar_stat:.4f}\"\n                \n                results_text += f\"McNemar's Test: {model1_name} vs {model2_name}\\n\"\n                results_text += f\"  Contingency Table:\\n\"\n                results_text += f\"    Both correct: {both_correct}\\n\"\n                results_text += f\"    {model1_name} only: {model1_only}\\n\"\n                results_text += f\"    {model2_name} only: {model2_only}\\n\"\n                results_text += f\"    Both wrong: {both_wrong}\\n\"\n                results_text += f\"  Chi-square statistic: {mcnemar_stat:.4f}\\n\"\n                results_text += f\"  P-value: {p_value:.4f}\\n\"\n                \n                if p_value < 0.05:\n                    results_text += f\"  Result: Significant difference (p < 0.05)\\n\"\n                else:\n                    results_text += f\"  Result: No significant difference (p >= 0.05)\\n\"\n                results_text += \"\\n\"\n        \n        # Bootstrap confidence intervals for AUC\n        results_text += \"BOOTSTRAP CONFIDENCE INTERVALS (AUC)\\n\" + \"-\"*40 + \"\\n\"\n        \n        for model_name, data in self.predictions.items():\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                # Bootstrap sampling\n                n_bootstrap = 1000\n                bootstrap_aucs = []\n                \n                for _ in range(n_bootstrap):\n                    # Sample with replacement\n                    indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n                    y_true_boot = y_true[indices]\n                    y_proba_boot = y_proba[indices]\n                    \n                    if len(np.unique(y_true_boot)) > 1:\n                        auc_boot = roc_auc_score(y_true_boot, y_proba_boot)\n                        bootstrap_aucs.append(auc_boot)\n                \n                if bootstrap_aucs:\n                    ci_lower = np.percentile(bootstrap_aucs, 2.5)\n                    ci_upper = np.percentile(bootstrap_aucs, 97.5)\n                    mean_auc = np.mean(bootstrap_aucs)\n                    \n                    results_text += f\"{model_name}:\\n\"\n                    results_text += f\"  Mean AUC: {mean_auc:.4f}\\n\"\n                    results_text += f\"  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\\n\\n\"\n        \n        # Save results\n        with open(f'{self.save_dir}/statistical_tests.txt', 'w') as f:\n            f.write(results_text)\n        \n        # Create visualization of statistical tests\n        plt.figure(figsize=(10, 8))\n        \n        # Plot AUC with confidence intervals\n        model_names = []\n        auc_values = []\n        ci_lowers = []\n        ci_uppers = []\n        \n        for model_name, data in self.predictions.items():\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                # Bootstrap for CI\n                n_bootstrap = 1000\n                bootstrap_aucs = []\n                \n                for _ in range(n_bootstrap):\n                    indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n                    y_true_boot = y_true[indices]\n                    y_proba_boot = y_proba[indices]\n                    \n                    if len(np.unique(y_true_boot)) > 1:\n                        auc_boot = roc_auc_score(y_true_boot, y_proba_boot)\n                        bootstrap_aucs.append(auc_boot)\n                \n                if bootstrap_aucs:\n                    model_names.append(model_name)\n                    auc_values.append(np.mean(bootstrap_aucs))\n                    ci_lowers.append(np.percentile(bootstrap_aucs, 2.5))\n                    ci_uppers.append(np.percentile(bootstrap_aucs, 97.5))\n        \n        if model_names:\n            y_pos = np.arange(len(model_names))\n            \n            plt.errorbar(auc_values, y_pos, \n                        xerr=[np.array(auc_values) - np.array(ci_lowers), \n                              np.array(ci_uppers) - np.array(auc_values)],\n                        fmt='o', capsize=5, capthick=2, markersize=8)\n            \n            plt.yticks(y_pos, model_names)\n            plt.xlabel('AUC Score')\n            plt.title('Model Performance with 95% Bootstrap Confidence Intervals')\n            plt.grid(True, alpha=0.3)\n            \n            # Add vertical line at 0.5 (random performance)\n            plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n            plt.legend()\n            \n            plt.tight_layout()\n            plt.savefig(f'{self.save_dir}/statistical_comparison.png', dpi=300, bbox_inches='tight')\n            plt.close()\n    \n    def generate_comprehensive_report(self):\n        print(\"Generating comprehensive report...\")\n        \n        html_content = \"\"\"\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <title>Model Evaluation Report</title>\n            <style>\n                body { font-family: Arial, sans-serif; margin: 20px; }\n                .header { background-color: #f0f0f0; padding: 20px; border-radius: 10px; }\n                .section { margin: 20px 0; padding: 15px; border-left: 4px solid #007acc; }\n                .metrics-table { border-collapse: collapse; width: 100%; margin: 20px 0; }\n                .metrics-table th, .metrics-table td { border: 1px solid #ddd; padding: 8px; text-align: center; }\n                .metrics-table th { background-color: #f2f2f2; }\n                .image-container { text-align: center; margin: 20px 0; }\n                .best-score { background-color: #90EE90; }\n                .worst-score { background-color: #FFB6C1; }\n            </style>\n        </head>\n        <body>\n        \"\"\"\n        \n        # Header\n        html_content += f\"\"\"\n        <div class=\"header\">\n            <h1>Advanced Model Evaluation Report</h1>\n            <p><strong>Generated on:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n            <p><strong>Number of Models:</strong> {len(self.predictions)}</p>\n            <p><strong>Dataset Size:</strong> {len(list(self.predictions.values())[0]['labels'])}</p>\n        </div>\n        \"\"\"\n        \n        # Executive Summary\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Executive Summary</h2>\n        \"\"\"\n        \n        # Find best performing model\n        best_model = None\n        best_auc = 0\n        \n        for model_name in self.results:\n            if self.results[model_name]['roc_auc'] > best_auc:\n                best_auc = self.results[model_name]['roc_auc']\n                best_model = model_name\n        \n        if best_model:\n            html_content += f\"\"\"\n            <p><strong>Best Performing Model:</strong> {best_model}</p>\n            <p><strong>Best AUC Score:</strong> {best_auc:.4f}</p>\n            <p><strong>Best F1 Score:</strong> {self.results[best_model]['f1_score']:.4f}</p>\n            <p><strong>Best Accuracy:</strong> {self.results[best_model]['accuracy']:.4f}</p>\n            \"\"\"\n        \n        html_content += \"</div>\"\n        \n        # Detailed Metrics Table\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Detailed Performance Metrics</h2>\n            <table class=\"metrics-table\">\n                <tr>\n                    <th>Model</th>\n                    <th>Accuracy</th>\n                    <th>Precision</th>\n                    <th>Recall</th>\n                    <th>F1 Score</th>\n                    <th>AUC</th>\n                    <th>Specificity</th>\n                    <th>Matthews Corr</th>\n                </tr>\n        \"\"\"\n        \n        # Find best/worst for each metric\n        metrics_to_highlight = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'specificity', 'matthews_corr']\n        best_values = {}\n        worst_values = {}\n        \n        for metric in metrics_to_highlight:\n            values = [self.results[model][metric] for model in self.results]\n            best_values[metric] = max(values)\n            worst_values[metric] = min(values)\n        \n        for model_name in self.results:\n            html_content += f\"<tr><td><strong>{model_name}</strong></td>\"\n            \n            for metric in metrics_to_highlight:\n                value = self.results[model_name][metric]\n                css_class = \"\"\n                \n                if value == best_values[metric] and len(self.results) > 1:\n                    css_class = \"best-score\"\n                elif value == worst_values[metric] and len(self.results) > 1:\n                    css_class = \"worst-score\"\n                \n                html_content += f'<td class=\"{css_class}\">{value:.4f}</td>'\n            \n            html_content += \"</tr>\"\n        \n        html_content += \"</table></div>\"\n        \n        # Visualizations\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Visualizations</h2>\n        \"\"\"\n        \n        # List all generated plots\n        plot_files = [\n            'performance_dashboard.png',\n            'roc_comparison.png', \n            'pr_comparison.png',\n            'calibration_plots.png',\n            'confusion_matrices.png',\n            'threshold_analysis.png',\n            'prediction_distributions.png',\n            'error_analysis.png',\n            'statistical_comparison.png'\n        ]\n        \n        for plot_file in plot_files:\n            if os.path.exists(f'{self.save_dir}/{plot_file}'):\n                html_content += f\"\"\"\n                <div class=\"image-container\">\n                    <h3>{plot_file.replace('_', ' ').replace('.png', '').title()}</h3>\n                    <img src=\"{plot_file}\" style=\"max-width: 100%; height: auto;\" alt=\"{plot_file}\">\n                </div>\n                \"\"\"\n        \n        html_content += \"</div>\"\n        \n        # Interactive Plots\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Interactive Visualizations</h2>\n            <p>The following interactive plots have been generated:</p>\n            <ul>\n                <li><a href=\"interactive_roc.html\">Interactive ROC Curves</a></li>\n                <li><a href=\"interactive_threshold.html\">Interactive Threshold Analysis</a></li>\n                <li><a href=\"interactive_calibration.html\">Interactive Calibration Plot</a></li>\n            </ul>\n        </div>\n        \"\"\"\n        \n        # Model Recommendations\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Recommendations</h2>\n        \"\"\"\n        \n        if best_model and self.results[best_model]['roc_auc'] > 0.8:\n            html_content += f\"<p><strong>{best_model}</strong> shows excellent performance with AUC > 0.8. This model is ready for deployment consideration.</p>\"\n        elif best_model and self.results[best_model]['roc_auc'] > 0.7:\n            html_content += f\"<p><strong>{best_model}</strong> shows good performance with AUC > 0.7, but there's room for improvement.</p>\"\n        else:\n            html_content += \"<p>All models show suboptimal performance (AUC < 0.7). Consider:</p>\"\n            html_content += \"<ul><li>Collecting more training data</li><li>Feature engineering</li><li>Different model architectures</li><li>Hyperparameter tuning</li></ul>\"\n        \n        # Check for overfitting signs\n        for model_name in self.results:\n            if 'calibration_error' in self.results[model_name]:\n                cal_error = self.results[model_name]['calibration_error']\n                if cal_error > 0.1:\n                    html_content += f\"<p><strong>{model_name}</strong> shows poor calibration (error: {cal_error:.3f}). Consider calibration techniques.</p>\"\n        \n        html_content += \"</div>\"\n        \n        html_content += \"\"\"\n        </body>\n        </html>\n        \"\"\"\n        \n        # Save HTML report\n        with open(f'{self.save_dir}/comprehensive_report.html', 'w') as f:\n            f.write(html_content)\n        \n        print(f\"Comprehensive report saved to {self.save_dir}/comprehensive_report.html\")\n    \n    def run_complete_evaluation(self, model_class):\n        print(\"Starting comprehensive model evaluation...\")\n        print(\"=\"*60)\n        \n        # Load models and generate predictions\n        self.load_model_predictions(model_class)\n        \n        if not self.predictions:\n            print(\"No valid predictions generated. Check model paths and data loader.\")\n            return None\n        \n        # Calculate comprehensive metrics\n        self.calculate_comprehensive_metrics()\n        \n        # Create visualizations\n        self.create_comprehensive_visualizations()\n        \n        # Generate report\n        self.generate_comprehensive_report()\n        \n        # Save results to JSON\n        results_for_json = {}\n        for model_name in self.results:\n            results_for_json[model_name] = {}\n            for metric, value in self.results[model_name].items():\n                if isinstance(value, (int, float, np.integer, np.floating)):\n                    results_for_json[model_name][metric] = float(value)\n                else:\n                    results_for_json[model_name][metric] = str(value)\n        \n        with open(f'{self.save_dir}/detailed_results.json', 'w') as f:\n            json.dump(results_for_json, f, indent=2)\n        \n        print(f\"\\nEvaluation completed successfully!\")\n        print(f\"Results saved in: {self.save_dir}/\")\n        print(f\"Open {self.save_dir}/comprehensive_report.html for full report\")\n        \n        return self.results\n\n\n# Usage Example\ndef run_evaluation_pipeline(model_paths, data_loader, model_class, device, save_dir=\"evaluation_results\"):\n    \"\"\"\n    Run the complete evaluation pipeline\n    \n    Args:\n        model_paths: List of model checkpoint paths or single path\n        data_loader: PyTorch DataLoader with test/validation data\n        model_class: Model class (not instantiated)\n        device: torch.device\n        save_dir: Directory to save results\n    \n    Returns:\n        Dictionary with evaluation results\n    \"\"\"\n    \n    # Create evaluator\n    evaluator = AdvancedModelEvaluator(\n        model_paths=model_paths,\n        data_loader=data_loader,\n        device=device,\n        class_names=['No Aneurysm', 'Aneurysm'],\n        save_dir=save_dir\n    )\n    \n    # Run complete evaluation\n    results = evaluator.run_complete_evaluation(model_class)\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}