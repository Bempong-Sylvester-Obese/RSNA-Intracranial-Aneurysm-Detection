{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**IMPORTS AND TPU SET UP**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pydicom\nfrom pydicom.errors import InvalidDicomError\nimport nibabel as nib\nimport cv2\nfrom scipy import ndimage\nfrom tqdm import tqdm\nimport warnings\nimport gc\nimport time\nimport signal\nimport sys\nfrom contextlib import contextmanager\nwarnings.filterwarnings('ignore')\n\n# TPU-SPECIFIC SETUP\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.utils.utils as xu\n    TPU_AVAILABLE = True\n    print(\"TPU libraries loaded successfully\")\nexcept ImportError:\n    TPU_AVAILABLE = False\n    print(\"TPU libraries not available, falling back to GPU/CPU\")\n\n#TPU Environment Setup\nif TPU_AVAILABLE:\n    os.environ[\"XLA_USE_BF16\"] = \"1\"  # Enable bfloat16 for TPU efficiency\n    os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\"  # 100MB tensor allocator\n    os.environ[\"TPU_NUM_DEVICES\"] = \"8\"  # v3-8 = 8 cores\nelse:\n    # GPU fallback settings\n    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False\n\n# Reduce thread contention (for TPU)\nos.environ[\"ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS\"] = \"1\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nprint(f\"TPU Available: {TPU_AVAILABLE}\")\nif TPU_AVAILABLE:\n    print(f\"TPU Devices: {xm.xrt_world_size()}\")\n\n# TIMEOUT AND HANGING PREVENTION\nclass TimeoutError(Exception):\n    pass\n\n@contextmanager\ndef timeout(duration):\n    def timeout_handler(signum, frame):\n        raise TimeoutError(f\"Operation timed out after {duration} seconds\")\n    \n    # Set the signal handler\n    if hasattr(signal, 'SIGALRM'):  # Unix systems\n        signal.signal(signal.SIGALRM, timeout_handler)\n        signal.alarm(duration)\n        try:\n            yield\n        finally:\n            signal.alarm(0)\n    else:\n        yield\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TPU OPTMIZED CONFIGURATION AND TRAINING PIPELINE**","metadata":{}},{"cell_type":"code","source":"class TPUConfig:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    LOCALIZER_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n    \n    # TPU-OPTIMIZED SIZES - Critical for v3-8\n    STAGE1_TARGET_SIZE = (16, 32, 32)  # Increased for TPU efficiency\n    STAGE1_BATCH_SIZE = 8 if TPU_AVAILABLE else 2 \n    STAGE1_EPOCHS = 10 \n    STAGE1_LR = 1e-3 if TPU_AVAILABLE else 3e-4\n    \n    # TPU Device Setup\n    if TPU_AVAILABLE:\n        DEVICE = xm.xla_device()\n        print(f\"Using TPU device: {DEVICE}\")\n    else:\n        DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using fallback device: {DEVICE}\")\n    \n    MIXED_PRECISION = True\n    N_FOLDS = 3\n    \n    # Constants\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n    TARGET_COL = 'Aneurysm Present'\n    \n    # Debug settings\n    DEBUG_MODE = False\n    DEBUG_SAMPLES = 100 \n    \n    # TPU-specific settings\n    TPU_SYNC_FREQUENCY = 10  # Sync every N batches\n    CHECKPOINT_FREQUENCY = 3  # Save every N epochs\n    MAX_TIMEOUT_SECONDS = 3600  # 1 hour timeout per epoch\n\nprint(f\"✅ TPU Configuration loaded - Device: {TPUConfig.DEVICE}\")\n\n# TPU-OPTIMIZED 3D UNET\nclass TPUOptimized3DUNet(nn.Module):\n    def __init__(self, spatial_dims=3, in_channels=1, out_channels=16, \n                 features=(16, 32, 64, 32), dropout=0.1):\n        super().__init__()\n        \n        self.features = features\n        self.dropout = dropout\n        \n        # Simplified Encoder - TPU-friendly operations\n        self.encoder_blocks = nn.ModuleList()\n        prev_channels = in_channels\n        \n        for feature_count in features:\n            # Single conv block per level for TPU efficiency\n            block = nn.Sequential(\n                nn.Conv3d(prev_channels, feature_count, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm3d(feature_count),\n                nn.ReLU(inplace=True),\n                nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n            )\n            self.encoder_blocks.append(block)\n            prev_channels = feature_count\n        \n        # Simplified downsampling\n        self.downsample_layers = nn.ModuleList([\n            nn.MaxPool3d(kernel_size=2, stride=2) \n            for _ in range(min(2, len(features) - 1))  # Limit levels\n        ])\n        \n        # Simplified Decoder\n        self.upsample_layers = nn.ModuleList()\n        self.decoder_blocks = nn.ModuleList()\n        \n        # Build decoder (reverse order)\n        decoder_features = list(reversed(features))\n        \n        for i in range(min(2, len(decoder_features) - 1)):\n            current_features = decoder_features[i]\n            next_features = decoder_features[i + 1]\n            \n            # Upsample layer\n            upsample = nn.ConvTranspose3d(\n                current_features, next_features,\n                kernel_size=2, stride=2, bias=False\n            )\n            self.upsample_layers.append(upsample)\n            \n            # Decoder block with skip connection\n            decoder_block = nn.Sequential(\n                nn.Conv3d(next_features * 2, next_features, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm3d(next_features),\n                nn.ReLU(inplace=True),\n                nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n            )\n            self.decoder_blocks.append(decoder_block)\n        \n        # Final output layer\n        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1, bias=True)\n        \n        # Initialize weights for TPU stability\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        skip_connections = []\n        \n        # Encoder path\n        for i, encoder_block in enumerate(self.encoder_blocks):\n            x = encoder_block(x)\n            skip_connections.append(x)\n            \n            if i < len(self.downsample_layers):\n                x = self.downsample_layers[i](x)\n        \n        # Decoder path\n        skip_connections = skip_connections[:-1]  # Remove last (bottleneck)\n        skip_connections.reverse()\n        \n        for i, (upsample_layer, decoder_block) in enumerate(zip(self.upsample_layers, self.decoder_blocks)):\n            x = upsample_layer(x)\n            \n            if i < len(skip_connections):\n                skip = skip_connections[i]\n                \n                # Handle size mismatch with interpolation\n                if x.shape[2:] != skip.shape[2:]:\n                    x = nn.functional.interpolate(\n                        x, size=skip.shape[2:], \n                        mode='trilinear', align_corners=False\n                    )\n                \n                x = torch.cat([x, skip], dim=1)\n            \n            x = decoder_block(x)\n        \n        x = self.final_conv(x)\n        return x\n\n# TPU-OPTIMIZED DICOM PROCESSOR\nclass TPUDICOMProcessor:\n    def __init__(self, target_size=None):\n        self.target_size = target_size or TPUConfig.STAGE1_TARGET_SIZE\n        self.max_files = 15  # reduced for TPU memory\n        \n    def load_dicom_series(self, series_path):\n        try:\n            if not os.path.exists(series_path):\n                return self._get_dummy_volume()\n                \n            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if not dicom_files:\n                return self._get_dummy_volume()\n            \n            # file limited for TPU memory\n            step_size = max(1, len(dicom_files) // self.max_files)\n            selected_files = dicom_files[::step_size][:self.max_files]\n            \n            pixel_arrays = []\n            target_slice_shape = (96, 96)  # Reduced for TPU memory\n            \n            for file_name in selected_files:\n                try:\n                    with timeout(30):  # 30 second timeout per file\n                        ds = pydicom.dcmread(\n                            os.path.join(series_path, file_name), \n                            force=True, \n                            stop_before_pixels=False\n                        )\n                        \n                        if hasattr(ds, 'pixel_array'):\n                            arr = ds.pixel_array.astype(np.float32)\n                            \n                            if arr.ndim == 2:\n                                # Resize for memory efficiency\n                                if arr.shape != target_slice_shape:\n                                    arr = cv2.resize(arr, target_slice_shape, interpolation=cv2.INTER_LINEAR)\n                                pixel_arrays.append(arr)\n                            elif arr.ndim == 3:\n                                # Handle 3D volumes - take middle slice\n                                middle_slice = arr[arr.shape[0] // 2]\n                                if middle_slice.shape != target_slice_shape:\n                                    middle_slice = cv2.resize(middle_slice, target_slice_shape, interpolation=cv2.INTER_LINEAR)\n                                pixel_arrays.append(middle_slice)\n                        \n                        # Critical: immediate cleanup\n                        del ds\n                        \n                except TimeoutError:\n                    print(f\"Timeout loading {file_name}\")\n                    continue\n                except Exception as e:\n                    print(f\"Error loading {file_name}: {str(e)[:100]}\")\n                    continue\n            \n            if not pixel_arrays:\n                return self._get_dummy_volume()\n            \n            # Create volume and clean up\n            volume = np.stack(pixel_arrays, axis=0).astype(np.float32)\n            del pixel_arrays  # Immediate cleanup\n            \n            # Preprocess\n            volume = self._preprocess_volume(volume)\n            \n            return volume\n            \n        except Exception as e:\n            print(f\"Failed to load series {os.path.basename(series_path)}: {str(e)[:100]}\")\n            return self._get_dummy_volume()\n    \n    def _get_dummy_volume(self):\n        \"\"\"Return dummy volume for failed loads\"\"\"\n        return np.zeros(self.target_size, dtype=np.float32)\n    \n    def _preprocess_volume(self, volume):\n        # Clip extreme outliers\n        p1, p99 = np.percentile(volume, [2, 98])  # Less aggressive clipping\n        volume = np.clip(volume, p1, p99)\n        \n        # Normalize\n        if p99 > p1:\n            volume = (volume - p1) / (p99 - p1)\n        else:\n            volume = np.zeros_like(volume)\n        \n        # Resize to target shape\n        if volume.shape != self.target_size:\n            zoom_factors = [self.target_size[i] / volume.shape[i] for i in range(3)]\n            volume = ndimage.zoom(volume, zoom_factors, order=1)\n        \n        # Ensure correct data type and range\n        volume = np.clip(volume, 0, 1).astype(np.float32)\n        \n        return volume\n\n# TPU-OPTIMIZED DATASET\nclass TPUSegmentationDataset(Dataset):\n    def __init__(self, df, series_dir, processor, mode='train'):\n        self.df = df.copy()  # Avoid reference issues\n        self.series_dir = series_dir\n        self.processor = processor\n        self.mode = mode\n        \n        print(f\"Created {mode} dataset with {len(self.df)} samples\")\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        \"\"\"Get item with robust error handling for TPU\"\"\"\n        max_retries = 2\n        \n        for attempt in range(max_retries + 1):\n            try:\n                return self._get_item_safe(idx)\n            except Exception as e:\n                if attempt == max_retries:\n                    print(f\"Failed to load item {idx} after {max_retries} retries\")\n                    return self._get_dummy_item()\n                else:\n                    print(f\"Retry {attempt + 1} for item {idx}: {str(e)[:50]}\")\n                    time.sleep(0.1)  # Brief pause before retry\n        \n        return self._get_dummy_item()\n    \n    def _get_item_safe(self, idx):\n        \"\"\"Safe item loading with timeout\"\"\"\n        with timeout(120):  # 2 minute timeout per item\n            row = self.df.iloc[idx]\n            series_id = row[TPUConfig.ID_COL]\n            series_path = os.path.join(self.series_dir, series_id)\n            \n            # Load volume\n            volume = self.processor.load_dicom_series(series_path)\n            \n            # Create simple mask (placeholder - in real scenario you'd load actual segmentation)\n            mask = self._create_dummy_mask(volume.shape, int(row[TPUConfig.TARGET_COL]))\n            \n            # Get classification label\n            has_aneurysm = float(row[TPUConfig.TARGET_COL])\n            \n            # Convert to tensors with correct dtype for TPU\n            volume_tensor = torch.from_numpy(volume).float().unsqueeze(0)  #channel dim\n            mask_tensor = torch.from_numpy(mask).float().unsqueeze(0)\n            \n            # Clean up numpy arrays\n            del volume, mask\n            \n            return {\n                'volume': volume_tensor,\n                'mask': mask_tensor,\n                'has_aneurysm': torch.tensor(has_aneurysm, dtype=torch.float32),\n                'series_id': series_id\n            }\n    \n    def _create_dummy_mask(self, shape, has_aneurysm):\n        mask = np.zeros(shape, dtype=np.float32)\n        \n        if has_aneurysm:\n            # Create a small central region as \"aneurysm\"\n            h, w, d = shape\n            center_h, center_w, center_d = h//2, w//2, d//2\n            size = min(h, w, d) // 4\n            \n            mask[\n                max(0, center_h-size//2):min(h, center_h+size//2),\n                max(0, center_w-size//2):min(w, center_w+size//2),\n                max(0, center_d-size//2):min(d, center_d+size//2)\n            ] = 1.0\n        \n        return mask\n    \n    def _get_dummy_item(self):\n        \"\"\"Return dummy item for failed loads\"\"\"\n        target_size = TPUConfig.STAGE1_TARGET_SIZE\n        return {\n            'volume': torch.zeros((1, *target_size), dtype=torch.float32),\n            'mask': torch.zeros((1, *target_size), dtype=torch.float32),\n            'has_aneurysm': torch.tensor(0.0, dtype=torch.float32),\n            'series_id': \"DUMMY_FAIL\"\n        }\n\n# TPU-OPTIMIZED MODEL\nclass TPUSegmentationModel(nn.Module):\n    def __init__(self, in_channels=1, seg_channels=1):\n        super().__init__()\n        \n        # Backbone U-Net\n        self.backbone = TPUOptimized3DUNet(\n            spatial_dims=3,\n            in_channels=in_channels,\n            out_channels=32,  # Feature channels\n            features=(16, 32, 64, 32),  # TPU-friendly sizes\n            dropout=0.1\n        )\n        \n        # Segmentation head\n        self.seg_head = nn.Conv3d(32, seg_channels, kernel_size=1, bias=True)\n        \n        # Classification head - for TPU\n        self.global_pool = nn.AdaptiveAvgPool3d(1)\n        self.classifier = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(64, 1)\n        )\n        \n        # Initialize for TPU stability\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        # Get features from backbone\n        features = self.backbone(x)\n        \n        # Segmentation output\n        seg_logits = self.seg_head(features)\n        \n        # Classification output\n        pooled_features = self.global_pool(features).flatten(1)\n        cls_logits = self.classifier(pooled_features)\n        \n        return seg_logits, cls_logits\n\n# TPU-OPTIMIZED TRAINING FUNCTIONS\ndef tpu_train_epoch(model, loader, optimizer, device, epoch):\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    \n    # Create progress bar\n    pbar = tqdm(loader, desc=f\"Epoch {epoch+1} Training\")\n    \n    for batch_idx, batch in enumerate(pbar):\n        try:\n            with timeout(300):  # 5-minute timeout per batch\n                # Move to device\n                volume = batch['volume'].to(device, non_blocking=True)\n                mask = batch['mask'].to(device, non_blocking=True)\n                has_aneurysm = batch['has_aneurysm'].to(device, non_blocking=True)\n                \n                # Clear gradients\n                optimizer.zero_grad()\n                \n                # Forward pass\n                seg_logits, cls_logits = model(volume)\n                \n                # Calculate losses\n                seg_loss = nn.functional.binary_cross_entropy_with_logits(seg_logits, mask)\n                cls_loss = nn.functional.binary_cross_entropy_with_logits(\n                    cls_logits.view(-1), has_aneurysm\n                )\n                total_loss_batch = seg_loss + cls_loss\n                \n                # Backward pass\n                total_loss_batch.backward()\n                \n                # Gradient clipping for TPU stability\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                \n                # TPU-specific step\n                if TPU_AVAILABLE:\n                    xm.optimizer_step(optimizer)  # TPU-optimized step\n                    if batch_idx % TPUConfig.TPU_SYNC_FREQUENCY == 0:\n                        xm.mark_step()  # Sync TPU cores\n                else:\n                    optimizer.step()\n                \n                # Update metrics\n                total_loss += total_loss_batch.item()\n                num_batches += 1\n                \n                # Update progress bar\n                pbar.set_postfix({\n                    'Loss': f'{total_loss_batch.item():.4f}',\n                    'Avg': f'{total_loss/num_batches:.4f}'\n                })\n                \n                # Memory cleanup for long training\n                if batch_idx % 20 == 0:\n                    if TPU_AVAILABLE:\n                        pass\n                    else:\n                        torch.cuda.empty_cache()\n                        gc.collect()\n        \n        except TimeoutError:\n            print(f\"Batch {batch_idx} timed out, skipping...\")\n            continue\n        except Exception as e:\n            print(f\"Error in batch {batch_idx}: {str(e)[:100]}\")\n            continue\n    \n    # Final TPU sync\n    if TPU_AVAILABLE:\n        xm.mark_step()\n    \n    avg_loss = total_loss / max(num_batches, 1)\n    return avg_loss\n\n\ndef tpu_validate_epoch(model, loader, device, epoch):\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    pbar = tqdm(loader, desc=f\"Epoch {epoch+1} Validation\")\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(pbar):\n            try:\n                with timeout(180):  # 3-minute timeout per validation batch\n                    # Move to device\n                    volume = batch['volume'].to(device, non_blocking=True)\n                    mask = batch['mask'].to(device, non_blocking=True)\n                    has_aneurysm = batch['has_aneurysm'].to(device, non_blocking=True)\n                    \n                    # Forward pass\n                    seg_logits, cls_logits = model(volume)\n                    \n                    # Calculate losses\n                    seg_loss = nn.functional.binary_cross_entropy_with_logits(seg_logits, mask)\n                    cls_loss = nn.functional.binary_cross_entropy_with_logits(\n                        cls_logits.view(-1), has_aneurysm\n                    )\n                    total_loss_batch = seg_loss + cls_loss\n                    \n                    total_loss += total_loss_batch.item()\n                    num_batches += 1\n                    \n                    # Update progress bar\n                    pbar.set_postfix({\n                        'Val Loss': f'{total_loss_batch.item():.4f}',\n                        'Avg': f'{total_loss/num_batches:.4f}'\n                    })\n            \n            except TimeoutError:\n                print(f\"Validation batch {batch_idx} timed out, skipping...\")\n                continue\n            except Exception as e:\n                print(f\"Validation error in batch {batch_idx}: {str(e)[:100]}\")\n                continue\n    \n    # Final TPU sync\n    if TPU_AVAILABLE:\n        xm.mark_step()\n    \n    avg_loss = total_loss / max(num_batches, 1)\n    return avg_loss\n\n# MAIN TPU TRAINING FUNCTION\ndef main_tpu_training():\n    print(f\"TPU BRAIN ANEURYSM TRAINING\")\n    print(f\"{'='*60}\")\n    print(f\"Device: {TPUConfig.DEVICE}\")\n    print(f\"TPU Available: {TPU_AVAILABLE}\")\n    \n    if TPU_AVAILABLE:\n        print(f\"TPU Cores: {xm.xrt_world_size()}\")\n        print(f\"Current TPU Core: {xm.get_ordinal()}\")\n    \n    try:\n        # Load and prepare data\n        print(\"Loading training data...\")\n        train_df = pd.read_csv(TPUConfig.TRAIN_CSV_PATH)\n        \n        if TPUConfig.DEBUG_MODE:\n            train_df = train_df.head(TPUConfig.DEBUG_SAMPLES)\n            print(f\"Debug mode: using {len(train_df)} samples\")\n        \n        print(f\"Training samples: {len(train_df)}\")\n        print(f\"Positive cases: {train_df[TPUConfig.TARGET_COL].sum()}\")\n        \n        # Simple train/val split\n        val_size = max(10, len(train_df) // 10)  # At least 10 samples for validation\n        val_df = train_df[:val_size].copy().reset_index(drop=True)\n        train_df = train_df[val_size:].copy().reset_index(drop=True)\n        \n        print(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n        \n        # Create datasets\n        processor = TPUDICOMProcessor()\n        train_dataset = TPUSegmentationDataset(train_df, TPUConfig.SERIES_DIR, processor, 'train')\n        val_dataset = TPUSegmentationDataset(val_df, TPUConfig.SERIES_DIR, processor, 'val')\n        \n        #data loaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=TPUConfig.STAGE1_BATCH_SIZE,\n            shuffle=True,\n            num_workers=0,\n            pin_memory=False,\n            drop_last=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=TPUConfig.STAGE1_BATCH_SIZE,\n            shuffle=False,\n            num_workers=0,\n            pin_memory=False,\n            drop_last=False\n        )\n        \n        print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n        \n        # Create model\n        print(\"Creating TPU-optimized model...\")\n        model = TPUSegmentationModel().to(TPUConfig.DEVICE)\n        \n        # Count parameters\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"Model parameters: {total_params:,} (trainable: {trainable_params:,})\")\n        \n        # Optimizer - TPU optimized\n        optimizer = optim.AdamW(\n            model.parameters(), \n            lr=TPUConfig.STAGE1_LR,\n            weight_decay=1e-4,\n            eps=1e-8  # TPU-friendly epsilon\n        )\n        \n        # Training loop with comprehensive hang prevention\n        best_val_loss = float('inf')\n        patience_counter = 0\n        patience_limit = 3\n        \n        print(f\"\\nStarting training for {TPUConfig.STAGE1_EPOCHS} epochs...\")\n        \n        for epoch in range(TPUConfig.STAGE1_EPOCHS):\n            print(f\"\\n{'='*20} EPOCH {epoch+1}/{TPUConfig.STAGE1_EPOCHS} {'='*20}\")\n            \n            epoch_start_time = time.time()\n            \n            try:\n                with timeout(TPUConfig.MAX_TIMEOUT_SECONDS):\n                    # Training phase\n                    print(\"Training phase...\")\n                    train_loss = tpu_train_epoch(model, train_loader, optimizer, TPUConfig.DEVICE, epoch)\n                    \n                    # Validation phase\n                    print(\"Validation phase...\")\n                    val_loss = tpu_validate_epoch(model, val_loader, TPUConfig.DEVICE, epoch)\n                    \n                    epoch_time = time.time() - epoch_start_time\n                    \n                    print(f\"\\nEpoch {epoch+1} Results:\")\n                    print(f\"   Train Loss: {train_loss:.4f}\")\n                    print(f\"   Val Loss: {val_loss:.4f}\")\n                    print(f\"   Time: {epoch_time:.1f}s\")\n                    \n                    # Save best model\n                    if val_loss < best_val_loss:\n                        best_val_loss = val_loss\n                        patience_counter = 0\n                        \n                        checkpoint = {\n                            'epoch': epoch,\n                            'model_state_dict': model.state_dict(),\n                            'optimizer_state_dict': optimizer.state_dict(),\n                            'train_loss': train_loss,\n                            'val_loss': val_loss,\n                            'best_val_loss': best_val_loss\n                        }\n                        \n                        torch.save(checkpoint, 'tpu_aneurysm_best.pth')\n                        print(f\"Saved best model (val_loss: {val_loss:.4f})\")\n                    else:\n                        patience_counter += 1\n                        print(f\"Patience: {patience_counter}/{patience_limit}\")\n                    \n                    # Regular checkpoint saving\n                    if (epoch + 1) % TPUConfig.CHECKPOINT_FREQUENCY == 0:\n                        checkpoint = {\n                            'epoch': epoch,\n                            'model_state_dict': model.state_dict(),\n                            'optimizer_state_dict': optimizer.state_dict(),\n                            'train_loss': train_loss,\n                            'val_loss': val_loss\n                        }\n                        torch.save(checkpoint, f'tpu_aneurysm_epoch_{epoch+1}.pth')\n                        print(f\"Saved checkpoint at epoch {epoch+1}\")\n                    \n                    # Early stopping\n                    if patience_counter >= patience_limit:\n                        print(f\"Early stopping at epoch {epoch+1}\")\n                        break\n                    \n                    # Memory cleanup\n                    if TPU_AVAILABLE:\n                        # Automatic TPU memory Management\n                        xm.mark_step()\n                    else:\n                        torch.cuda.empty_cache()\n                        gc.collect()\n            \n            except TimeoutError:\n                print(f\"Epoch {epoch+1} timed out after {TPUConfig.MAX_TIMEOUT_SECONDS}s\")\n                print(\"Saving emergency checkpoint...\")\n                \n                emergency_checkpoint = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'status': 'timeout_interrupted'\n                }\n                torch.save(emergency_checkpoint, f'tpu_aneurysm_emergency_epoch_{epoch+1}.pth')\n                \n                # Try to continue or break based on severity\n                if epoch >= 2:  # If progress is made\n                    print(\"Attempting to continue training...\")\n                    continue\n                else:\n                    print(\"Early epoch timeout, stopping training\")\n                    break\n                    \n            except Exception as e:\n                print(f\"Unexpected error in epoch {epoch+1}: {str(e)}\")\n                print(\"Saving emergency checkpoint...\")\n                \n                try:\n                    emergency_checkpoint = {\n                        'epoch': epoch,\n                        'model_state_dict': model.state_dict(),\n                        'status': 'error_interrupted',\n                        'error': str(e)\n                    }\n                    torch.save(emergency_checkpoint, f'tpu_aneurysm_error_epoch_{epoch+1}.pth')\n                except:\n                    print(\"Failed to save emergency checkpoint\")\n                \n                # Decide whether to continue or stop\n                if epoch >= 2:\n                    print(\"Attempting to recover and continue...\")\n                    time.sleep(10)  # Brief recovery pause\n                    continue\n                else:\n                    print(\"Critical error in early training, stopping\")\n                    break\n        \n        print(f\"\\nTraining completed!\")\n        print(f\"Best validation loss: {best_val_loss:.4f}\")\n        \n        # Final model save\n        final_checkpoint = {\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_val_loss': best_val_loss,\n            'total_epochs': epoch + 1,\n            'status': 'completed'\n        }\n        torch.save(final_checkpoint, 'tpu_aneurysm_final.pth')\n        print(\"Saved final model checkpoint\")\n        \n        return model, best_val_loss\n        \n    except Exception as e:\n        print(f\"Critical training error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# RUN TRAINING\nmodel, best_loss = main_tpu_training()\nprint(\"Expected training time: ? hours\")\nprint(\"Output: tpu_aneurysm_final.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model Evaluation and Performance**","metadata":{}},{"cell_type":"code","source":"def quick_tpu_evaluation(model_path='tpu_aneurysm_best.pth'):\n    print(\"TPU MODEL EVALUATION\")\n    print(\"=\"*50)\n    \n    try:\n        # Load test data\n        train_df = pd.read_csv(TPUConfig.TRAIN_CSV_PATH)\n        \n        # Create small test set\n        test_size = min(50, len(train_df) // 10)  # Small for quick eval\n        test_df = train_df.sample(n=test_size, random_state=42).reset_index(drop=True)\n        \n        print(f\"Test set: {len(test_df)} samples\")\n        print(f\"Positive cases: {test_df[TPUConfig.TARGET_COL].sum()}\")\n        \n        # Load model\n        model = TPUSegmentationModel().to(TPUConfig.DEVICE)\n        \n        if os.path.exists(model_path):\n            print(f\"Loading model from {model_path}\")\n            checkpoint = torch.load(model_path, map_location=TPUConfig.DEVICE)\n            \n            if 'model_state_dict' in checkpoint:\n                model.load_state_dict(checkpoint['model_state_dict'])\n                print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n                print(f\"Best val loss: {checkpoint.get('best_val_loss', 'unknown')}\")\n            else:\n                model.load_state_dict(checkpoint)\n        else:\n            print(f\"Model file {model_path} not found, using random weights\")\n        \n        # Create test dataset and loader\n        processor = TPUDICOMProcessor()\n        test_dataset = TPUSegmentationDataset(test_df, TPUConfig.SERIES_DIR, processor, 'test')\n        test_loader = DataLoader(\n            test_dataset, \n            batch_size=1,  # Small batch for evaluation\n            shuffle=False, \n            num_workers=0\n        )\n        \n        # Evaluation\n        model.eval()\n        predictions = []\n        probabilities = []\n        true_labels = []\n        \n        print(\"Running evaluation...\")\n        with torch.no_grad():\n            for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n                try:\n                    with timeout(60):  # 1-minute timeout per batch\n                        volume = batch['volume'].to(TPUConfig.DEVICE, non_blocking=True)\n                        true_label = batch['has_aneurysm'].to(TPUConfig.DEVICE, non_blocking=True)\n                        \n                        # Forward pass\n                        seg_logits, cls_logits = model(volume)\n                        \n                        # Get predictions\n                        prob = torch.sigmoid(cls_logits).cpu().numpy()[0]\n                        pred = 1 if prob > 0.5 else 0\n                        \n                        probabilities.append(prob)\n                        predictions.append(pred)\n                        true_labels.append(true_label.cpu().numpy()[0])\n                        \n                        # Periodic TPU sync\n                        if TPU_AVAILABLE and batch_idx % 10 == 0:\n                            xm.mark_step()\n                \n                except TimeoutError:\n                    print(f\"Evaluation batch {batch_idx} timed out\")\n                    continue\n                except Exception as e:\n                    print(f\"Error in evaluation batch {batch_idx}: {str(e)[:50]}\")\n                    continue\n        \n        # Calculate metrics\n        predictions = np.array(predictions)\n        probabilities = np.array(probabilities)\n        true_labels = np.array(true_labels)\n        \n        if len(predictions) > 0:\n            from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n            \n            accuracy = accuracy_score(true_labels, predictions)\n            \n            if len(np.unique(true_labels)) > 1:  # Check if both classes are available \n                auc = roc_auc_score(true_labels, probabilities)\n                print(f\"AUC-ROC: {auc:.3f}\")\n            else:\n                print(\"Only one class in test set, cannot calculate AUC\")\n            \n            print(f\"Accuracy: {accuracy:.3f}\")\n            print(f\"Predictions made: {len(predictions)}\")\n            \n            print(\"\\nClassification Report:\")\n            print(classification_report(true_labels, predictions, target_names=['No Aneurysm', 'Aneurysm']))\n            \n            # Sample predictions\n            print(f\"\\nSample Predictions:\")\n            for i in range(min(5, len(predictions))):\n                status = \"✅\" if predictions[i] == true_labels[i] else \"❌\"\n                print(f\"{status} True: {int(true_labels[i])}, Pred: {int(predictions[i])}, Prob: {probabilities[i]:.3f}\")\n        \n        else:\n            print(\"No successful predictions made\")\n        \n        print(f\"\\nEvaluation completed!\")\n        \n        return {\n            'predictions': predictions,\n            'probabilities': probabilities,\n            'true_labels': true_labels,\n            'accuracy': accuracy_score(true_labels, predictions) if len(predictions) > 0 else 0\n        }\n        \n    except Exception as e:\n        print(f\"Evaluation error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}