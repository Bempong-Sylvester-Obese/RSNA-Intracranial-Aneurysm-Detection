{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceType":"competition","sourceId":99552,"databundleVersionId":13441085}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**IMPORTS AND TPU SET UP**","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport pydicom\nfrom pydicom.errors import InvalidDicomError\nimport nibabel as nib\nimport cv2\nfrom scipy import ndimage\nfrom scipy.stats import zscore\nfrom tqdm.auto import tqdm\nimport warnings\nimport gc\nimport time\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (accuracy_score, roc_auc_score, classification_report, \n                           confusion_matrix, precision_recall_curve, roc_curve,\n                           f1_score, precision_score, recall_score)\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport albumentations as A\nfrom collections import Counter\nimport logging\nwarnings.filterwarnings('ignore')\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\ndef install_required_packages():\n    packages = [\n        'pydicom',\n        'nibabel', \n        'opencv-python',\n        'scikit-learn',\n        'albumentations',\n        'seaborn',\n        'matplotlib'\n    ]\n    \n    for package in packages:\n        try:\n            __import__(package.replace('-', '_'))\n            print(f\"{package} already installed\")\n        except ImportError:\n            print(f\"Installing {package}...\")\n            try:\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n                print(f\"{package} installed successfully\")\n            except Exception as e:\n                print(f\"Failed to install {package}: {e}\")\n\nprint(\"Checking and installing required packages...\")\ninstall_required_packages()\n\n# TPU-SPECIFIC SETUP\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    TPU_AVAILABLE = True\n    print(\"TPU libraries loaded successfully\")\nexcept ImportError:\n    TPU_AVAILABLE = False\n    print(\"TPU libraries not available, falling back to GPU/CPU\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TPU OPTMIZED CONFIGURATION AND TRAINING PIPELINE**","metadata":{}},{"cell_type":"code","source":"class Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    \n    # Model parameters - More reasonable sizes for better learning\n    TARGET_SIZE = (32, 224, 224)  # Reduced depth, increased spatial resolution\n    BATCH_SIZE = 1 if TPU_AVAILABLE else 2  # Smaller batch for stability\n    EPOCHS = 30  # More epochs for better convergence\n    LEARNING_RATE = 5e-5  # Lower learning rate\n    WEIGHT_DECAY = 1e-5\n    WARMUP_EPOCHS = 3\n    \n    # Advanced training parameters\n    GRADIENT_CLIP = 1.0\n    ACCUMULATION_STEPS = 4  # Gradient accumulation\n    EARLY_STOPPING_PATIENCE = 8\n    SCHEDULER_PATIENCE = 4\n    \n    # Device setup\n    if TPU_AVAILABLE:\n        DEVICE = xm.xla_device()\n        print(f\"Using TPU device: {DEVICE}\")\n    else:\n        DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {DEVICE}\")\n    \n    # Data parameters\n    ID_COL = 'SeriesInstanceUID'\n    TARGET_COL = 'Aneurysm Present'\n    \n    # Debug settings\n    DEBUG_MODE = False  # Turn off for full training\n    DEBUG_SAMPLES = 500\n    \n    # Cross-validation\n    N_FOLDS = 5\n    CURRENT_FOLD = 0  \n    \n    # Data augmentation\n    USE_AUGMENTATION = True\n    AUGMENTATION_PROB = 0.3\n\nclass AdvancedDICOMProcessor:\n    def __init__(self, target_size=None, hu_window=(-1000, 1000)):\n        self.target_size = target_size or Config.TARGET_SIZE\n        self.max_slices = self.target_size[0]\n        self.hu_window = hu_window\n        self.stats = {'processed': 0, 'failed': 0, 'dummy': 0}\n        \n    def load_dicom_series(self, series_path):\n        try:\n            if not os.path.exists(series_path):\n                logger.warning(f\"Series path does not exist: {series_path}\")\n                self.stats['dummy'] += 1\n                return self._get_dummy_volume()\n            \n            dicom_files = [f for f in os.listdir(series_path) \n                          if f.endswith('.dcm') or f.endswith('.DCM')]\n            \n            if not dicom_files:\n                logger.warning(f\"No DICOM files found in: {series_path}\")\n                self.stats['dummy'] += 1\n                return self._get_dummy_volume()\n            \n            # Load all DICOM files with metadata\n            dicom_data = []\n            for file_name in dicom_files:\n                try:\n                    file_path = os.path.join(series_path, file_name)\n                    ds = pydicom.dcmread(file_path, force=True)\n                    \n                    if hasattr(ds, 'pixel_array') and hasattr(ds, 'ImagePositionPatient'):\n                        # Get slice location for proper ordering\n                        slice_location = float(getattr(ds, 'SliceLocation', 0))\n                        instance_number = int(getattr(ds, 'InstanceNumber', 0))\n                        \n                        dicom_data.append({\n                            'dataset': ds,\n                            'slice_location': slice_location,\n                            'instance_number': instance_number,\n                            'filename': file_name\n                        })\n                except Exception as e:\n                    logger.warning(f\"Failed to load {file_name}: {e}\")\n                    continue\n            \n            if not dicom_data:\n                self.stats['dummy'] += 1\n                return self._get_dummy_volume()\n            \n            # Sort by slice location, then by instance number\n            dicom_data.sort(key=lambda x: (x['slice_location'], x['instance_number']))\n            \n            # Extract pixel arrays with proper preprocessing\n            pixel_arrays = []\n            target_shape = self.target_size[1:]  # H, W\n            \n            for item in dicom_data:\n                try:\n                    ds = item['dataset']\n                    arr = ds.pixel_array.astype(np.float32)\n                    \n                    # Handle different DICOM formats\n                    if arr.ndim == 2:\n                        # Apply DICOM transformations\n                        arr = self._apply_dicom_transforms(ds, arr)\n                        \n                        # Resize to target shape\n                        if arr.shape != target_shape:\n                            arr = cv2.resize(arr, (target_shape[1], target_shape[0]), \n                                           interpolation=cv2.INTER_AREA)\n                        \n                        pixel_arrays.append(arr)\n                        \n                    elif arr.ndim == 3:\n                        # Multi-slice DICOM - take all slices\n                        for slice_idx in range(arr.shape[0]):\n                            slice_arr = arr[slice_idx]\n                            slice_arr = self._apply_dicom_transforms(ds, slice_arr)\n                            \n                            if slice_arr.shape != target_shape:\n                                slice_arr = cv2.resize(slice_arr, (target_shape[1], target_shape[0]), \n                                                     interpolation=cv2.INTER_AREA)\n                            \n                            pixel_arrays.append(slice_arr)\n                    \n                    # Clean up\n                    del ds\n                    \n                except Exception as e:\n                    logger.warning(f\"Failed to process slice: {e}\")\n                    continue\n            \n            if not pixel_arrays:\n                self.stats['dummy'] += 1\n                return self._get_dummy_volume()\n            \n            # Handle slice selection and padding\n            volume = self._create_volume_from_slices(pixel_arrays)\n            \n            # Advanced preprocessing pipeline\n            volume = self._advanced_preprocessing(volume)\n            \n            self.stats['processed'] += 1\n            return volume\n            \n        except Exception as e:\n            logger.error(f\"Critical error processing {series_path}: {e}\")\n            self.stats['failed'] += 1\n            return self._get_dummy_volume()\n    \n    def _apply_dicom_transforms(self, ds, arr):\n        # Apply rescale slope and intercept if available\n        if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n            slope = float(ds.RescaleSlope)\n            intercept = float(ds.RescaleIntercept)\n            arr = arr * slope + intercept\n        \n        # Apply window/level if available\n        if hasattr(ds, 'WindowCenter') and hasattr(ds, 'WindowWidth'):\n            try:\n                center = float(ds.WindowCenter) if not isinstance(ds.WindowCenter, list) else float(ds.WindowCenter[0])\n                width = float(ds.WindowWidth) if not isinstance(ds.WindowWidth, list) else float(ds.WindowWidth[0])\n                \n                min_val = center - width / 2\n                max_val = center + width / 2\n                arr = np.clip(arr, min_val, max_val)\n            except:\n                pass\n        \n        return arr\n    \n    def _create_volume_from_slices(self, pixel_arrays):\n        # If we have more slices than needed, select evenly distributed ones\n        if len(pixel_arrays) > self.max_slices:\n            indices = np.linspace(0, len(pixel_arrays)-1, self.max_slices, dtype=int)\n            selected_arrays = [pixel_arrays[i] for i in indices]\n        else:\n            selected_arrays = pixel_arrays.copy()\n        \n        # Pad with edge replication if needed\n        while len(selected_arrays) < self.max_slices:\n            if selected_arrays:\n                # Replicate edge slices\n                selected_arrays.insert(0, selected_arrays[0])  # Add at beginning\n                if len(selected_arrays) < self.max_slices:\n                    selected_arrays.append(selected_arrays[-1])  # Add at end\n            else:\n                # Create zero slices\n                selected_arrays.append(np.zeros(self.target_size[1:], dtype=np.float32))\n        \n        # Ensure exact number of slices\n        selected_arrays = selected_arrays[:self.max_slices]\n        \n        # Stack into volume\n        volume = np.stack(selected_arrays, axis=0).astype(np.float32)\n        \n        return volume\n    \n    def _advanced_preprocessing(self, volume):\n        # 1. Outlier removal using percentiles\n        p1, p99 = np.percentile(volume, [1, 99])\n        volume = np.clip(volume, p1, p99)\n        \n        # 2. Z-score normalization per slice to handle varying contrast\n        normalized_slices = []\n        for slice_idx in range(volume.shape[0]):\n            slice_data = volume[slice_idx]\n            if slice_data.std() > 0:\n                normalized_slice = zscore(slice_data.flatten()).reshape(slice_data.shape)\n                normalized_slice = np.clip(normalized_slice, -3, 3)  # Clip extreme values\n            else:\n                normalized_slice = slice_data\n            normalized_slices.append(normalized_slice)\n        \n        volume = np.stack(normalized_slices, axis=0)\n        \n        # 3. Min-max normalization to [0, 1]\n        vol_min, vol_max = volume.min(), volume.max()\n        if vol_max > vol_min:\n            volume = (volume - vol_min) / (vol_max - vol_min)\n        \n        # 4. Slight Gaussian smoothing for noise reduction\n        volume = ndimage.gaussian_filter(volume, sigma=0.5)\n        \n        return volume.astype(np.float32)\n    \n    def _get_dummy_volume(self):\n        \"\"\"Generate realistic dummy volume\"\"\"\n        # Create a volume that resembles brain tissue\n        volume = np.random.normal(0.3, 0.15, self.target_size).astype(np.float32)\n        \n        # Add some structure\n        for i in range(self.target_size[0]):\n            # Add circular structures (resembling brain anatomy)\n            center_y, center_x = self.target_size[1]//2, self.target_size[2]//2\n            y, x = np.ogrid[:self.target_size[1], :self.target_size[2]]\n            mask = (x - center_x)**2 + (y - center_y)**2 < (min(self.target_size[1], self.target_size[2])//3)**2\n            volume[i][mask] += 0.2\n        \n        volume = np.clip(volume, 0, 1)\n        return volume\n    \n    def get_stats(self):\n        return self.stats\n\nclass VolumetricAugmentation:\n    \"\"\"3D-aware data augmentation\"\"\"\n    \n    def __init__(self, prob=0.3):\n        self.prob = prob\n        \n        # 2D augmentations (applied slice-wise)\n        self.slice_transform = A.Compose([\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n            A.GaussianBlur(blur_limit=3, p=0.2),\n        ], p=self.prob)\n    \n    def __call__(self, volume):\n        if np.random.random() > self.prob:\n            return volume\n        \n        augmented_volume = np.zeros_like(volume)\n        \n        for i in range(volume.shape[0]):\n            slice_2d = volume[i]\n            \n            # Convert to uint8 for albumentations\n            slice_uint8 = (slice_2d * 255).astype(np.uint8)\n            \n            # Apply 2D augmentation\n            transformed = self.slice_transform(image=slice_uint8)\n            augmented_slice = transformed['image'].astype(np.float32) / 255.0\n            \n            augmented_volume[i] = augmented_slice\n        \n        # 3D-specific augmentations\n        if np.random.random() < 0.3:\n            # Random slice dropout\n            n_dropout = np.random.randint(1, min(4, volume.shape[0]//4))\n            dropout_indices = np.random.choice(volume.shape[0], n_dropout, replace=False)\n            for idx in dropout_indices:\n                augmented_volume[idx] = 0\n        \n        if np.random.random() < 0.2:\n            # Flip along depth axis\n            augmented_volume = np.flip(augmented_volume, axis=0)\n        \n        return augmented_volume\n\nclass ImprovedAneurysmDataset(Dataset):\n    def __init__(self, df, series_dir, processor, augmentation=None, mode='train'):\n        self.df = df.copy().reset_index(drop=True)\n        self.series_dir = series_dir\n        self.processor = processor\n        self.augmentation = augmentation\n        self.mode = mode\n        \n        # Validate data\n        self._validate_data()\n        \n        # Create cache for loaded volumes (memory permitting)\n        self.cache = {}\n        self.use_cache = len(self.df) < 100  # Only cache small datasets\n        \n        logger.info(f\"Dataset created with {len(self.df)} samples\")\n        logger.info(f\"Positive cases: {self.df[Config.TARGET_COL].sum()}\")\n        logger.info(f\"Mode: {mode}, Use cache: {self.use_cache}\")\n        \n    def _validate_data(self):\n        # Check for missing values\n        missing_ids = self.df[Config.ID_COL].isnull().sum()\n        missing_labels = self.df[Config.TARGET_COL].isnull().sum()\n        \n        if missing_ids > 0:\n            logger.warning(f\"Found {missing_ids} missing IDs\")\n            self.df = self.df.dropna(subset=[Config.ID_COL])\n        \n        if missing_labels > 0:\n            logger.warning(f\"Found {missing_labels} missing labels\")\n            self.df = self.df.dropna(subset=[Config.TARGET_COL])\n        \n        # Validate label values\n        unique_labels = self.df[Config.TARGET_COL].unique()\n        logger.info(f\"Unique labels: {unique_labels}\")\n        \n        if not all(label in [0, 1] for label in unique_labels):\n            logger.error(f\"Invalid labels found: {unique_labels}\")\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        try:\n            row = self.df.iloc[idx]\n            series_id = str(row[Config.ID_COL])\n            label = float(row[Config.TARGET_COL])\n            \n            # Try cache first\n            if self.use_cache and series_id in self.cache:\n                volume = self.cache[series_id]\n            else:\n                series_path = os.path.join(self.series_dir, series_id)\n                volume = self.processor.load_dicom_series(series_path)\n                \n                if self.use_cache:\n                    self.cache[series_id] = volume\n            \n            # Apply augmentation for training\n            if self.augmentation is not None and self.mode == 'train':\n                volume = self.augmentation(volume)\n            \n            # Convert to tensor\n            volume_tensor = torch.from_numpy(volume).float().unsqueeze(0)  # Add channel dim\n            label_tensor = torch.tensor(label, dtype=torch.float32)\n            \n            return {\n                'volume': volume_tensor,\n                'label': label_tensor,\n                'series_id': series_id,\n                'idx': idx\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error loading sample {idx}: {e}\")\n            # Return dummy data with correct label\n            return {\n                'volume': torch.zeros((1, *Config.TARGET_SIZE), dtype=torch.float32),\n                'label': torch.tensor(0.0, dtype=torch.float32),\n                'series_id': f\"DUMMY_{idx}\",\n                'idx': idx\n            }\n\nclass ResidualBlock3D(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock3D, self).__init__()\n        \n        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, \n                              stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(out_channels)\n        \n        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, \n                              stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(out_channels)\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        \n    def forward(self, x):\n        residual = x\n        \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        \n        out = self.conv2(out)\n        out = self.bn2(out)\n        \n        if self.downsample is not None:\n            residual = self.downsample(x)\n        \n        out += residual\n        out = self.relu(out)\n        \n        return out\n\nclass AttentionBlock3D(nn.Module):\n    def __init__(self, channels):\n        super(AttentionBlock3D, self).__init__()\n        self.channels = channels\n        \n        # Channel attention\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool3d(1),\n            nn.Conv3d(channels, channels // 8, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(channels // 8, channels, 1),\n            nn.Sigmoid()\n        )\n        \n        # Spatial attention\n        self.spatial_attention = nn.Sequential(\n            nn.Conv3d(2, 1, kernel_size=7, padding=3),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        # Channel attention\n        ca = self.channel_attention(x)\n        x = x * ca\n        \n        # Spatial attention\n        avg_pool = torch.mean(x, dim=1, keepdim=True)\n        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n        sa_input = torch.cat([avg_pool, max_pool], dim=1)\n        sa = self.spatial_attention(sa_input)\n        x = x * sa\n        \n        return x\n\nclass AdvancedAneurysmNet(nn.Module):\n    \"\"\"Advanced 3D CNN with residual blocks and attention\"\"\"\n    \n    def __init__(self, in_channels=1, num_classes=1, dropout_rate=0.3):\n        super(AdvancedAneurysmNet, self).__init__()\n        \n        # Initial convolution\n        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        \n        # Residual layers\n        self.layer1 = self._make_layer(64, 64, 2)\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n        \n        # Attention mechanisms\n        self.attention1 = AttentionBlock3D(128)\n        self.attention2 = AttentionBlock3D(256)\n        self.attention3 = AttentionBlock3D(512)\n        \n        # Global pooling and classifier\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Multi-layer classifier with batch norm\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate * 0.5),\n            \n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate * 0.3),\n            \n            nn.Linear(64, num_classes)\n        )\n        \n        self._initialize_weights()\n    \n    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(out_channels)\n            )\n        \n        layers = []\n        layers.append(ResidualBlock3D(in_channels, out_channels, stride, downsample))\n        \n        for _ in range(1, blocks):\n            layers.append(ResidualBlock3D(out_channels, out_channels))\n        \n        return nn.Sequential(*layers)\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        # Initial layers\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        # Residual layers with attention\n        x = self.layer1(x)\n        \n        x = self.layer2(x)\n        x = self.attention1(x)\n        \n        x = self.layer3(x)\n        x = self.attention2(x)\n        \n        x = self.layer4(x)\n        x = self.attention3(x)\n        \n        # Global pooling and classification\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        \n        return x\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, pos_weight=None):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.pos_weight = pos_weight\n    \n    def forward(self, inputs, targets):\n        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n            inputs.view(-1), targets, pos_weight=self.pos_weight, reduction='none'\n        )\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n        return focal_loss.mean()\n\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n    \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n        \n        return self.counter >= self.patience\n    \n    def save_checkpoint(self, model):\n        if self.restore_best_weights:\n            self.best_weights = model.state_dict().copy()\n    \n    def restore(self, model):\n        if self.best_weights is not None:\n            model.load_state_dict(self.best_weights)\n\nclass MetricsTracker:\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.predictions = []\n        self.probabilities = []\n        self.labels = []\n        self.losses = []\n    \n    def update(self, preds, probs, labels, loss):\n        self.predictions.extend(preds)\n        self.probabilities.extend(probs)\n        self.labels.extend(labels)\n        self.losses.append(loss)\n    \n    def compute_metrics(self):\n        if not self.labels:\n            return {}\n        \n        y_true = np.array(self.labels)\n        y_pred = np.array(self.predictions)\n        y_proba = np.array(self.probabilities)\n        \n        metrics = {\n            'loss': np.mean(self.losses),\n            'accuracy': accuracy_score(y_true, y_pred),\n            'precision': precision_score(y_true, y_pred, zero_division=0),\n            'recall': recall_score(y_true, y_pred, zero_division=0),\n            'f1': f1_score(y_true, y_pred, zero_division=0),\n        }\n        \n        # Add AUC if we have both classes\n        if len(np.unique(y_true)) > 1:\n            metrics['auc'] = roc_auc_score(y_true, y_proba)\n        else:\n            metrics['auc'] = 0.5\n        \n        return metrics\n\ndef create_balanced_sampler(dataset):\n    \"\"\"Create weighted sampler for balanced training\"\"\"\n    targets = [dataset[i]['label'].item() for i in range(len(dataset))]\n    class_counts = Counter(targets)\n    \n    # Calculate weights\n    total_samples = len(targets)\n    weights = []\n    for target in targets:\n        weights.append(total_samples / (len(class_counts) * class_counts[target]))\n    \n    return WeightedRandomSampler(weights, len(weights))\n\ndef train_epoch(model, loader, optimizer, criterion, device, epoch, accumulation_steps=1):\n    \"\"\"Enhanced training loop with gradient accumulation\"\"\"\n    model.train()\n    metrics_tracker = MetricsTracker()\n    \n    progress_bar = tqdm(\n        enumerate(loader),\n        total=len(loader),\n        desc=f\"Training Epoch {epoch+1}\",\n        leave=False\n    )\n    \n    optimizer.zero_grad()\n    \n    for batch_idx, batch in progress_bar:\n        volume = batch['volume'].to(device, non_blocking=True)\n        label = batch['label'].to(device, non_blocking=True)\n        \n        # Forward pass\n        outputs = model(volume)\n        loss = criterion(outputs, label)\n        \n        # Scale loss for gradient accumulation\n        loss = loss / accumulation_steps\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), Config.GRADIENT_CLIP)\n        \n        # Update weights every accumulation_steps\n        if (batch_idx + 1) % accumulation_steps == 0:\n            if TPU_AVAILABLE:\n                xm.optimizer_step(optimizer)\n            else:\n                optimizer.step()\n            optimizer.zero_grad()\n        \n        # Compute metrics\n        with torch.no_grad():\n            probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n            preds = (probs > 0.5).astype(int)\n            labels_np = label.cpu().numpy()\n            \n            metrics_tracker.update(preds, probs, labels_np, loss.item() * accumulation_steps)\n        \n        # Update progress bar\n        current_metrics = metrics_tracker.compute_metrics()\n        progress_bar.set_postfix({\n            'Loss': f'{current_metrics.get(\"loss\", 0):.4f}',\n            'Acc': f'{current_metrics.get(\"accuracy\", 0):.3f}',\n            'AUC': f'{current_metrics.get(\"auc\", 0):.3f}'\n        })\n        \n        # TPU synchronization\n        if TPU_AVAILABLE and batch_idx % 10 == 0:\n            xm.mark_step()\n    \n    # Final optimizer step if needed\n    if len(loader) % accumulation_steps != 0:\n        if TPU_AVAILABLE:\n            xm.optimizer_step(optimizer)\n        else:\n            optimizer.step()\n        optimizer.zero_grad()\n    \n    progress_bar.close()\n    \n    if TPU_AVAILABLE:\n        xm.mark_step()\n    \n    return metrics_tracker.compute_metrics()\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    metrics_tracker = MetricsTracker()\n    \n    progress_bar = tqdm(\n        enumerate(loader),\n        total=len(loader),\n        desc=\"Validation\",\n        leave=False\n    )\n    \n    with torch.no_grad():\n        for batch_idx, batch in progress_bar:\n            volume = batch['volume'].to(device, non_blocking=True)\n            label = batch['label'].to(device, non_blocking=True)\n            \n            outputs = model(volume)\n            loss = criterion(outputs, label)\n            \n            # Compute metrics\n            probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n            preds = (probs > 0.5).astype(int)\n            labels_np = label.cpu().numpy()\n            \n            metrics_tracker.update(preds, probs, labels_np, loss.item())\n            \n            # Update progress bar\n            current_metrics = metrics_tracker.compute_metrics()\n            progress_bar.set_postfix({\n                'Val Loss': f'{current_metrics.get(\"loss\", 0):.4f}',\n                'Val Acc': f'{current_metrics.get(\"accuracy\", 0):.3f}',\n                'Val AUC': f'{current_metrics.get(\"auc\", 0):.3f}'\n            })\n    \n    progress_bar.close()\n    \n    if TPU_AVAILABLE:\n        xm.mark_step()\n    \n    return metrics_tracker.compute_metrics()\n\ndef plot_training_history(history):\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    \n    metrics = ['loss', 'accuracy', 'auc', 'precision', 'recall', 'f1']\n    \n    for i, metric in enumerate(metrics):\n        ax = axes[i // 3, i % 3]\n        \n        if f'train_{metric}' in history and f'val_{metric}' in history:\n            ax.plot(history[f'train_{metric}'], label=f'Train {metric.upper()}', marker='o')\n            ax.plot(history[f'val_{metric}'], label=f'Val {metric.upper()}', marker='s')\n            ax.set_title(f'{metric.upper()} History')\n            ax.set_xlabel('Epoch')\n            ax.set_ylabel(metric.upper())\n            ax.legend()\n            ax.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef comprehensive_evaluation(model, loader, device, save_plots=True):\n    \"\"\"Comprehensive model evaluation with detailed metrics and plots\"\"\"\n    model.eval()\n    \n    all_predictions = []\n    all_probabilities = []\n    all_labels = []\n    \n    print(\"ðŸ” Running comprehensive evaluation...\")\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Evaluating\"):\n            volume = batch['volume'].to(device)\n            label = batch['label']\n            \n            outputs = model(volume)\n            probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n            preds = (probs > 0.5).astype(int)\n            \n            all_probabilities.extend(probs)\n            all_predictions.extend(preds)\n            all_labels.extend(label.numpy())\n    \n    # Convert to numpy arrays\n    y_true = np.array(all_labels)\n    y_pred = np.array(all_predictions)\n    y_proba = np.array(all_probabilities)\n    \n    # Calculate comprehensive metrics\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, zero_division=0),\n        'recall': recall_score(y_true, y_pred, zero_division=0),\n        'f1': f1_score(y_true, y_pred, zero_division=0),\n        'auc': roc_auc_score(y_true, y_proba) if len(np.unique(y_true)) > 1 else 0.5\n    }\n    \n    print(\"\\nCOMPREHENSIVE EVALUATION RESULTS\")\n    print(\"=\"*50)\n    for metric, value in metrics.items():\n        print(f\"{metric.upper():>12}: {value:.4f}\")\n    \n    # Detailed classification report\n    print(f\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, \n                              target_names=['No Aneurysm', 'Aneurysm'],\n                              zero_division=0))\n    \n    if save_plots:\n        # Create comprehensive plots\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        \n        # 1. Confusion Matrix\n        cm = confusion_matrix(y_true, y_pred)\n        sns.heatmap(cm, annot=True, fmt='d', ax=axes[0,0], \n                   xticklabels=['No Aneurysm', 'Aneurysm'],\n                   yticklabels=['No Aneurysm', 'Aneurysm'])\n        axes[0,0].set_title('Confusion Matrix')\n        axes[0,0].set_ylabel('True Label')\n        axes[0,0].set_xlabel('Predicted Label')\n        \n        # 2. ROC Curve\n        if len(np.unique(y_true)) > 1:\n            fpr, tpr, _ = roc_curve(y_true, y_proba)\n            axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, \n                          label=f'ROC curve (AUC = {metrics[\"auc\"]:.3f})')\n            axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n            axes[0,1].set_xlim([0.0, 1.0])\n            axes[0,1].set_ylim([0.0, 1.05])\n            axes[0,1].set_xlabel('False Positive Rate')\n            axes[0,1].set_ylabel('True Positive Rate')\n            axes[0,1].set_title('ROC Curve')\n            axes[0,1].legend(loc=\"lower right\")\n            axes[0,1].grid(True)\n        \n        # 3. Precision-Recall Curve\n        if len(np.unique(y_true)) > 1:\n            precision, recall, _ = precision_recall_curve(y_true, y_proba)\n            axes[1,0].plot(recall, precision, color='blue', lw=2)\n            axes[1,0].set_xlabel('Recall')\n            axes[1,0].set_ylabel('Precision')\n            axes[1,0].set_title('Precision-Recall Curve')\n            axes[1,0].grid(True)\n        \n        # 4. Probability Distribution\n        axes[1,1].hist(y_proba[y_true == 0], bins=30, alpha=0.7, \n                      label='No Aneurysm', density=True)\n        axes[1,1].hist(y_proba[y_true == 1], bins=30, alpha=0.7, \n                      label='Aneurysm', density=True)\n        axes[1,1].set_xlabel('Predicted Probability')\n        axes[1,1].set_ylabel('Density')\n        axes[1,1].set_title('Probability Distribution')\n        axes[1,1].legend()\n        axes[1,1].grid(True)\n        \n        plt.tight_layout()\n        plt.savefig('evaluation_results.png', dpi=300, bbox_inches='tight')\n        plt.show()\n    \n    return metrics\n\ndef cross_validation_training(train_df, series_dir):\n    \"\"\"K-fold cross-validation training\"\"\"\n    skf = StratifiedKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=42)\n    fold_results = []\n    \n    processor = AdvancedDICOMProcessor()\n    augmentation = VolumetricAugmentation(prob=Config.AUGMENTATION_PROB) if Config.USE_AUGMENTATION else None\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df[Config.TARGET_COL])):\n        print(f\"\\n{'='*20} FOLD {fold + 1}/{Config.N_FOLDS} {'='*20}\")\n        \n        if Config.CURRENT_FOLD >= 0 and fold != Config.CURRENT_FOLD:\n            print(f\"Skipping fold {fold + 1}\")\n            continue\n        \n        fold_train_df = train_df.iloc[train_idx].reset_index(drop=True)\n        fold_val_df = train_df.iloc[val_idx].reset_index(drop=True)\n        \n        print(f\"Train: {len(fold_train_df)}, Val: {len(fold_val_df)}\")\n        print(f\"Train pos: {fold_train_df[Config.TARGET_COL].sum()}, \"\n              f\"Val pos: {fold_val_df[Config.TARGET_COL].sum()}\")\n        \n        # Create datasets\n        train_dataset = ImprovedAneurysmDataset(\n            fold_train_df, series_dir, processor, augmentation, mode='train'\n        )\n        val_dataset = ImprovedAneurysmDataset(\n            fold_val_df, series_dir, processor, mode='val'\n        )\n        \n        # Create balanced sampler\n        sampler = create_balanced_sampler(train_dataset)\n        \n        # Create data loaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=Config.BATCH_SIZE,\n            sampler=sampler,\n            num_workers=0,\n            pin_memory=True if Config.DEVICE.type == 'cuda' else False,\n            persistent_workers=False\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=Config.BATCH_SIZE,\n            shuffle=False,\n            num_workers=0,\n            pin_memory=True if Config.DEVICE.type == 'cuda' else False\n        )\n        \n        # Calculate class weights\n        pos_count = fold_train_df[Config.TARGET_COL].sum()\n        neg_count = len(fold_train_df) - pos_count\n        pos_weight = torch.tensor([neg_count / pos_count if pos_count > 0 else 1.0])\n        \n        print(f\"Positive weight: {pos_weight.item():.2f}\")\n        \n        # Create model\n        model = AdvancedAneurysmNet().to(Config.DEVICE)\n        \n        # Loss function and optimizer\n        criterion = FocalLoss(alpha=1, gamma=2, pos_weight=pos_weight.to(Config.DEVICE))\n        \n        optimizer = optim.AdamW(\n            model.parameters(),\n            lr=Config.LEARNING_RATE,\n            weight_decay=Config.WEIGHT_DECAY,\n            betas=(0.9, 0.999),\n            eps=1e-8\n        )\n        \n        # Learning rate scheduler\n        scheduler = optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=Config.LEARNING_RATE * 10,\n            epochs=Config.EPOCHS,\n            steps_per_epoch=len(train_loader),\n            pct_start=0.3,\n            div_factor=25,\n            final_div_factor=1000\n        )\n        \n        # Early stopping\n        early_stopping = EarlyStopping(patience=Config.EARLY_STOPPING_PATIENCE, min_delta=0.001)\n        \n        # Training history\n        history = {\n            'train_loss': [], 'val_loss': [],\n            'train_accuracy': [], 'val_accuracy': [],\n            'train_auc': [], 'val_auc': [],\n            'train_precision': [], 'val_precision': [],\n            'train_recall': [], 'val_recall': [],\n            'train_f1': [], 'val_f1': []\n        }\n        \n        best_val_auc = 0\n        \n        print(f\"\\nStarting training for fold {fold + 1}...\")\n        \n        for epoch in range(Config.EPOCHS):\n            print(f\"\\nEpoch {epoch+1}/{Config.EPOCHS}\")\n            print(\"-\" * 30)\n            \n            # Training\n            train_metrics = train_epoch(\n                model, train_loader, optimizer, criterion, \n                Config.DEVICE, epoch, Config.ACCUMULATION_STEPS\n            )\n            \n            # Validation\n            val_metrics = validate_epoch(model, val_loader, criterion, Config.DEVICE)\n            \n            # Update scheduler\n            if isinstance(scheduler, optim.lr_scheduler.OneCycleLR):\n                # OneCycleLR is stepped per batch, not per epoch\n                pass\n            else:\n                scheduler.step(val_metrics['loss'])\n            \n            # Record history\n            for metric in ['loss', 'accuracy', 'auc', 'precision', 'recall', 'f1']:\n                history[f'train_{metric}'].append(train_metrics.get(metric, 0))\n                history[f'val_{metric}'].append(val_metrics.get(metric, 0))\n            \n            # Print metrics\n            print(f\"Train - Loss: {train_metrics['loss']:.4f}, \"\n                  f\"Acc: {train_metrics['accuracy']:.4f}, \"\n                  f\"AUC: {train_metrics['auc']:.4f}\")\n            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, \"\n                  f\"Acc: {val_metrics['accuracy']:.4f}, \"\n                  f\"AUC: {val_metrics['auc']:.4f}\")\n            \n            # Save best model\n            if val_metrics['auc'] > best_val_auc:\n                best_val_auc = val_metrics['auc']\n                \n                checkpoint = {\n                    'fold': fold,\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'train_metrics': train_metrics,\n                    'val_metrics': val_metrics,\n                    'best_val_auc': best_val_auc,\n                    'history': history,\n                    'config': Config.__dict__.copy()\n                }\n                \n                torch.save(checkpoint, f'fold_{fold}_best_model.pth')\n                print(f\"Saved best model (AUC: {val_metrics['auc']:.4f})\")\n            \n            # Early stopping\n            if early_stopping(val_metrics['loss'], model):\n                print(f\"Early stopping at epoch {epoch+1}\")\n                early_stopping.restore(model)\n                break\n            \n            # Memory cleanup\n            if not TPU_AVAILABLE:\n                torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Final evaluation\n        print(f\"\\nðŸ” Final evaluation for fold {fold + 1}\")\n        final_metrics = comprehensive_evaluation(model, val_loader, Config.DEVICE)\n        \n        fold_result = {\n            'fold': fold + 1,\n            'best_val_auc': best_val_auc,\n            'final_metrics': final_metrics,\n            'history': history\n        }\n        fold_results.append(fold_result)\n        \n        # Plot training history for this fold\n        plot_training_history(history)\n        plt.title(f'Fold {fold + 1} Training History')\n        plt.savefig(f'fold_{fold}_history.png', dpi=300, bbox_inches='tight')\n        \n        print(f\"Fold {fold + 1} completed!\")\n        print(f\"Best AUC: {best_val_auc:.4f}\")\n        \n        # Print processor stats\n        processor_stats = processor.get_stats()\n        print(f\"ðŸ“Š DICOM Processing Stats: {processor_stats}\")\n        \n        if Config.CURRENT_FOLD >= 0:\n            break  # Only train specified fold\n    \n    return fold_results\n\ndef main_training():\n    print(\"ADVANCED ANEURYSM DETECTION TRAINING\")\n    print(\"=\" * 60)\n    print(f\"Device: {Config.DEVICE}\")\n    print(f\"TPU Available: {TPU_AVAILABLE}\")\n    print(f\"Debug Mode: {Config.DEBUG_MODE}\")\n    \n    # Load and validate data\n    print(\"\\nLoading training data...\")\n    try:\n        train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n        print(f\"Loaded {len(train_df)} samples from CSV\")\n    except Exception as e:\n        print(f\"Error loading CSV: {e}\")\n        return None\n    \n    # Data validation and preprocessing\n    print(\"\\nData validation...\")\n    print(f\"Original samples: {len(train_df)}\")\n    print(f\"Columns: {list(train_df.columns)}\")\n    \n    # Check required columns\n    if Config.ID_COL not in train_df.columns:\n        print(f\"Missing ID column: {Config.ID_COL}\")\n        return None\n    \n    if Config.TARGET_COL not in train_df.columns:\n        print(f\"Missing target column: {Config.TARGET_COL}\")\n        return None\n    \n    # Clean data\n    initial_len = len(train_df)\n    train_df = train_df.dropna(subset=[Config.ID_COL, Config.TARGET_COL])\n    print(f\"After removing NaN: {len(train_df)} (removed {initial_len - len(train_df)})\")\n    \n    # Debug mode\n    if Config.DEBUG_MODE:\n        train_df = train_df.sample(n=min(Config.DEBUG_SAMPLES, len(train_df)), \n                                  random_state=42).reset_index(drop=True)\n        print(f\"Debug mode: using {len(train_df)} samples\")\n    \n    # Class distribution\n    class_dist = train_df[Config.TARGET_COL].value_counts().sort_index()\n    print(f\"\\nClass Distribution:\")\n    for label, count in class_dist.items():\n        percentage = (count / len(train_df)) * 100\n        print(f\"   Class {label}: {count} samples ({percentage:.1f}%)\")\n    \n    # Check if data is too imbalanced\n    min_class_count = class_dist.min()\n    if min_class_count < 10:\n        print(f\"Warning: Very few samples in minority class ({min_class_count})\")\n        print(\"Consider collecting more data or adjusting class weights\")\n    \n    # Verify series directories exist\n    print(f\"\\nVerifying series directories...\")\n    existing_series = 0\n    sample_check = train_df[Config.ID_COL].head(100)  # Check first 100\n    \n    for series_id in sample_check:\n        series_path = os.path.join(Config.SERIES_DIR, str(series_id))\n        if os.path.exists(series_path):\n            existing_series += 1\n    \n    existence_rate = (existing_series / len(sample_check)) * 100\n    print(f\"Series existence rate: {existence_rate:.1f}% ({existing_series}/{len(sample_check)} checked)\")\n    \n    if existence_rate < 50:\n        print(\"Warning: Many series directories are missing!\")\n        print(\"This may significantly impact training performance.\")\n    \n    # Start cross-validation training\n    print(f\"\\nStarting cross-validation training...\")\n    fold_results = cross_validation_training(train_df, Config.SERIES_DIR)\n    \n    # Summarize results\n    if fold_results:\n        print(f\"\\nCROSS-VALIDATION RESULTS SUMMARY\")\n        print(\"=\" * 50)\n        \n        all_aucs = [result['best_val_auc'] for result in fold_results]\n        mean_auc = np.mean(all_aucs)\n        std_auc = np.std(all_aucs)\n        \n        print(f\"Mean AUC: {mean_auc:.4f} Â± {std_auc:.4f}\")\n        print(f\"Best AUC: {max(all_aucs):.4f}\")\n        print(f\"Worst AUC: {min(all_aucs):.4f}\")\n        \n        for i, result in enumerate(fold_results):\n            print(f\"Fold {result['fold']}: AUC = {result['best_val_auc']:.4f}\")\n        \n        # Save results\n        results_summary = {\n            'mean_auc': mean_auc,\n            'std_auc': std_auc,\n            'fold_results': fold_results,\n            'config': Config.__dict__.copy()\n        }\n        \n        with open('cv_results.json', 'w') as f:\n            # Convert non-serializable objects to strings\n            serializable_results = json.dumps(results_summary, default=str, indent=2)\n            f.write(serializable_results)\n        \n        print(f\"\\nResults saved to cv_results.json\")\n        return fold_results\n    else:\n        print(\"No results to summarize\")\n        return None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    results = main_training()\n    \n    if results:\n        print(f\"\\nTraining completed successfully!\")\n        print(f\"Check the following files:\")\n        print(f\"   - fold_*_best_model.pth: Best model checkpoints\")\n        print(f\"   - fold_*_history.png: Training history plots\") \n        print(f\"   - evaluation_results.png: Evaluation plots\")\n        print(f\"   - cv_results.json: Cross-validation summary\")\n    else:\n        print(f\"\\nTraining failed. Please check the error messages above.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model Evaluation and Performance**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n    roc_curve, precision_recall_curve, confusion_matrix, classification_report,\n    average_precision_score, balanced_accuracy_score, matthews_corrcoef,\n    cohen_kappa_score, log_loss, brier_score_loss\n)\nfrom sklearn.calibration import calibration_curve\nfrom scipy import stats\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nfrom tqdm.auto import tqdm\nimport warnings\nimport json\nfrom datetime import datetime\nfrom collections import defaultdict\nimport itertools\nfrom scipy.stats import bootstrap\nwarnings.filterwarnings('ignore')\n\nclass AdvancedModelEvaluator:\n    \"\"\"\n    Comprehensive model evaluation suite with advanced metrics and visualizations\n    \"\"\"\n    \n    def __init__(self, model_paths, data_loader, device, class_names=None, save_dir=\"evaluation_results\"):\n        self.model_paths = model_paths if isinstance(model_paths, list) else [model_paths]\n        self.data_loader = data_loader\n        self.device = device\n        self.class_names = class_names or ['No Aneurysm', 'Aneurysm']\n        self.save_dir = save_dir\n        \n        # Create save directory\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # Storage for results\n        self.results = {}\n        self.predictions = {}\n        self.raw_outputs = {}\n        \n    def load_model_predictions(self, model_class):\n        \"\"\"Load models and generate predictions\"\"\"\n        print(\"Loading models and generating predictions...\")\n        \n        for i, model_path in enumerate(self.model_paths):\n            print(f\"\\nProcessing model: {model_path}\")\n            \n            # Load model\n            model = model_class().to(self.device)\n            \n            if os.path.exists(model_path):\n                checkpoint = torch.load(model_path, map_location=self.device)\n                model.load_state_dict(checkpoint['model_state_dict'])\n                print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n            else:\n                print(f\"Model file not found: {model_path}\")\n                continue\n            \n            # Generate predictions\n            model.eval()\n            predictions = []\n            probabilities = []\n            labels = []\n            logits = []\n            series_ids = []\n            \n            with torch.no_grad():\n                for batch in tqdm(self.data_loader, desc=f\"Model {i+1} Inference\"):\n                    volume = batch['volume'].to(self.device)\n                    label = batch['label']\n                    series_id = batch.get('series_id', [f'sample_{j}' for j in range(len(label))])\n                    \n                    outputs = model(volume)\n                    probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n                    preds = (probs > 0.5).astype(int)\n                    \n                    predictions.extend(preds)\n                    probabilities.extend(probs)\n                    labels.extend(label.numpy())\n                    logits.extend(outputs.cpu().numpy().flatten())\n                    series_ids.extend(series_id)\n            \n            model_name = f\"Model_{i+1}\" if len(self.model_paths) > 1 else \"Model\"\n            \n            self.predictions[model_name] = {\n                'predictions': np.array(predictions),\n                'probabilities': np.array(probabilities),\n                'labels': np.array(labels),\n                'logits': np.array(logits),\n                'series_ids': series_ids,\n                'model_path': model_path\n            }\n    \n    def calculate_comprehensive_metrics(self):\n        print(\"\\nCalculating comprehensive metrics...\")\n        \n        for model_name, data in self.predictions.items():\n            y_true = data['labels']\n            y_pred = data['predictions']\n            y_proba = data['probabilities']\n            y_logits = data['logits']\n            \n            # Basic classification metrics\n            basic_metrics = {\n                'accuracy': accuracy_score(y_true, y_pred),\n                'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n                'precision': precision_score(y_true, y_pred, zero_division=0),\n                'recall': recall_score(y_true, y_pred, zero_division=0),\n                'specificity': recall_score(1 - y_true, 1 - y_pred, zero_division=0),\n                'f1_score': f1_score(y_true, y_pred, zero_division=0),\n                'matthews_corr': matthews_corrcoef(y_true, y_pred),\n                'cohen_kappa': cohen_kappa_score(y_true, y_pred),\n            }\n            \n            # Probabilistic metrics\n            if len(np.unique(y_true)) > 1:\n                prob_metrics = {\n                    'roc_auc': roc_auc_score(y_true, y_proba),\n                    'pr_auc': average_precision_score(y_true, y_proba),\n                    'log_loss': log_loss(y_true, y_proba),\n                    'brier_score': brier_score_loss(y_true, y_proba),\n                }\n            else:\n                prob_metrics = {\n                    'roc_auc': 0.5,\n                    'pr_auc': np.mean(y_true),\n                    'log_loss': float('inf'),\n                    'brier_score': float('inf'),\n                }\n            \n            # Threshold-dependent metrics\n            threshold_metrics = self._calculate_threshold_metrics(y_true, y_proba)\n            \n            # Confidence and calibration metrics\n            calibration_metrics = self._calculate_calibration_metrics(y_true, y_proba)\n            \n            # Class-wise metrics\n            class_metrics = self._calculate_class_wise_metrics(y_true, y_pred, y_proba)\n            \n            # Ensemble metrics (if multiple models)\n            ensemble_metrics = {}\n            if len(self.predictions) > 1:\n                ensemble_metrics = self._calculate_ensemble_metrics()\n            \n            # Combine all metrics\n            all_metrics = {\n                **basic_metrics,\n                **prob_metrics,\n                **threshold_metrics,\n                **calibration_metrics,\n                **class_metrics,\n                **ensemble_metrics\n            }\n            \n            self.results[model_name] = all_metrics\n    \n    def _calculate_threshold_metrics(self, y_true, y_proba):\n        thresholds = np.arange(0.1, 1.0, 0.1)\n        threshold_results = {}\n        \n        for threshold in thresholds:\n            y_pred_thresh = (y_proba >= threshold).astype(int)\n            \n            if len(np.unique(y_pred_thresh)) > 1:\n                threshold_results[f'f1_thresh_{threshold:.1f}'] = f1_score(y_true, y_pred_thresh)\n                threshold_results[f'precision_thresh_{threshold:.1f}'] = precision_score(y_true, y_pred_thresh, zero_division=0)\n                threshold_results[f'recall_thresh_{threshold:.1f}'] = recall_score(y_true, y_pred_thresh, zero_division=0)\n        \n        # Find optimal threshold\n        if len(np.unique(y_true)) > 1:\n            fpr, tpr, thresholds_roc = roc_curve(y_true, y_proba)\n            optimal_idx = np.argmax(tpr - fpr)\n            optimal_threshold = thresholds_roc[optimal_idx]\n            \n            threshold_results['optimal_threshold'] = optimal_threshold\n            threshold_results['optimal_f1'] = f1_score(y_true, (y_proba >= optimal_threshold).astype(int))\n        \n        return threshold_results\n    \n    def _calculate_calibration_metrics(self, y_true, y_proba):\n        # Calibration curve\n        if len(np.unique(y_true)) > 1:\n            prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n            calibration_error = np.mean(np.abs(prob_true - prob_pred))\n            \n            # Expected Calibration Error (ECE)\n            bin_boundaries = np.linspace(0, 1, 11)\n            bin_lowers = bin_boundaries[:-1]\n            bin_uppers = bin_boundaries[1:]\n            \n            ece = 0\n            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n                in_bin = (y_proba > bin_lower) & (y_proba <= bin_upper)\n                prop_in_bin = in_bin.mean()\n                \n                if prop_in_bin > 0:\n                    accuracy_in_bin = y_true[in_bin].mean()\n                    avg_confidence_in_bin = y_proba[in_bin].mean()\n                    ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n        else:\n            calibration_error = float('inf')\n            ece = float('inf')\n        \n        # Confidence metrics\n        confidence_metrics = {\n            'mean_confidence': np.mean(y_proba),\n            'std_confidence': np.std(y_proba),\n            'confidence_range': np.ptp(y_proba),\n            'calibration_error': calibration_error,\n            'expected_calibration_error': ece,\n        }\n        \n        return confidence_metrics\n    \n    def _calculate_class_wise_metrics(self, y_true, y_pred, y_proba):\n        cm = confusion_matrix(y_true, y_pred)\n        \n        if cm.shape == (2, 2):\n            tn, fp, fn, tp = cm.ravel()\n            \n            class_metrics = {\n                'true_positives': tp,\n                'true_negatives': tn,\n                'false_positives': fp,\n                'false_negatives': fn,\n                'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,\n                'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n                'positive_predictive_value': tp / (tp + fp) if (tp + fp) > 0 else 0,\n                'negative_predictive_value': tn / (tn + fn) if (tn + fn) > 0 else 0,\n                'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0,\n                'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0,\n                'positive_likelihood_ratio': (tp/(tp+fn)) / (fp/(fp+tn)) if fp > 0 else float('inf'),\n                'negative_likelihood_ratio': (fn/(fn+tp)) / (tn/(tn+fp)) if tn > 0 else float('inf'),\n            }\n        else:\n            class_metrics = {}\n        \n        return class_metrics\n    \n    def _calculate_ensemble_metrics(self):\n        if len(self.predictions) < 2:\n            return {}\n        \n        # Average predictions across models\n        all_probas = []\n        all_preds = []\n        y_true = None\n        \n        for model_name, data in self.predictions.items():\n            all_probas.append(data['probabilities'])\n            all_preds.append(data['predictions'])\n            if y_true is None:\n                y_true = data['labels']\n        \n        # Ensemble by averaging probabilities\n        ensemble_proba = np.mean(all_probas, axis=0)\n        ensemble_pred = (ensemble_proba > 0.5).astype(int)\n        \n        # Ensemble by majority voting\n        majority_pred = np.round(np.mean(all_preds, axis=0)).astype(int)\n        \n        ensemble_metrics = {\n            'ensemble_accuracy': accuracy_score(y_true, ensemble_pred),\n            'ensemble_f1': f1_score(y_true, ensemble_pred),\n            'ensemble_auc': roc_auc_score(y_true, ensemble_proba) if len(np.unique(y_true)) > 1 else 0.5,\n            'majority_vote_accuracy': accuracy_score(y_true, majority_pred),\n            'majority_vote_f1': f1_score(y_true, majority_pred),\n        }\n        \n        # Model agreement\n        agreement_matrix = np.array(all_preds)\n        model_agreement = np.mean(np.std(agreement_matrix, axis=0) == 0)  # Fraction of samples where all models agree\n        \n        ensemble_metrics['model_agreement'] = model_agreement\n        \n        return ensemble_metrics\n    \n    def create_comprehensive_visualizations(self):\n        print(\"\\nCreating comprehensive visualizations...\")\n        \n        # Set style\n        plt.style.use('seaborn-v0_8')\n        colors = ['#2E8B57', '#DC143C', '#4169E1', '#FFD700', '#8A2BE2']\n        \n        # 1. Performance Overview Dashboard\n        self._create_performance_dashboard()\n        \n        # 2. ROC Curves Comparison\n        self._create_roc_comparison()\n        \n        # 3. Precision-Recall Curves\n        self._create_pr_comparison()\n        \n        # 4. Calibration Plots\n        self._create_calibration_plots()\n        \n        # 5. Confusion Matrices\n        self._create_confusion_matrices()\n        \n        # 6. Threshold Analysis\n        self._create_threshold_analysis()\n        \n        # 7. Prediction Distribution Analysis\n        self._create_prediction_distributions()\n        \n        # 8. Error Analysis\n        self._create_error_analysis()\n        \n        # 9. Interactive Plotly Visualizations\n        self._create_interactive_plots()\n        \n        # 10. Statistical Significance Tests\n        self._create_statistical_tests()\n    \n    def _create_performance_dashboard(self):\n        n_models = len(self.predictions)\n        fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n        fig.suptitle('Model Performance Dashboard', fontsize=16, fontweight='bold')\n        \n        # Key metrics comparison\n        metrics_to_plot = ['accuracy', 'f1_score', 'roc_auc', 'pr_auc', \n                          'precision', 'recall', 'specificity', 'matthews_corr']\n        \n        for i, metric in enumerate(metrics_to_plot):\n            ax = axes[i // 4, i % 4]\n            \n            values = [self.results[model][metric] for model in self.results.keys()]\n            model_names = list(self.results.keys())\n            \n            bars = ax.bar(model_names, values, color=colors[:len(values)])\n            ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontweight='bold')\n            ax.set_ylabel('Score')\n            ax.set_ylim(0, 1)\n            \n            # Add value labels on bars\n            for bar, value in zip(bars, values):\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                       f'{value:.3f}', ha='center', va='bottom')\n            \n            ax.tick_params(axis='x', rotation=45)\n        \n        # Remove empty subplots\n        for j in range(len(metrics_to_plot), 12):\n            axes[j // 4, j % 4].remove()\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/performance_dashboard.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_roc_comparison(self):\n        plt.figure(figsize=(10, 8))\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                fpr, tpr, _ = roc_curve(y_true, y_proba)\n                auc = roc_auc_score(y_true, y_proba)\n                \n                plt.plot(fpr, tpr, color=colors[i], lw=2,\n                        label=f'{model_name} (AUC = {auc:.3f})')\n        \n        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.8)\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate', fontsize=12)\n        plt.ylabel('True Positive Rate', fontsize=12)\n        plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n        plt.legend(loc=\"lower right\")\n        plt.grid(True, alpha=0.3)\n        \n        plt.savefig(f'{self.save_dir}/roc_comparison.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_pr_comparison(self):\n        plt.figure(figsize=(10, 8))\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                precision, recall, _ = precision_recall_curve(y_true, y_proba)\n                avg_precision = average_precision_score(y_true, y_proba)\n                \n                plt.plot(recall, precision, color=colors[i], lw=2,\n                        label=f'{model_name} (AP = {avg_precision:.3f})')\n        \n        # Baseline\n        baseline = np.mean([data['labels'] for data in self.predictions.values()][0])\n        plt.axhline(y=baseline, color='gray', linestyle='--', alpha=0.8, \n                   label=f'Baseline (AP = {baseline:.3f})')\n        \n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('Recall', fontsize=12)\n        plt.ylabel('Precision', fontsize=12)\n        plt.title('Precision-Recall Curve Comparison', fontsize=14, fontweight='bold')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.savefig(f'{self.save_dir}/pr_comparison.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_calibration_plots(self):\n        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Calibration curve\n        ax1 = axes[0]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n                ax1.plot(prob_pred, prob_true, marker='o', color=colors[i], \n                        label=model_name, linewidth=2, markersize=6)\n        \n        ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n        ax1.set_xlabel('Mean Predicted Probability')\n        ax1.set_ylabel('Fraction of Positives')\n        ax1.set_title('Calibration Plot (Reliability Diagram)')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Confidence histogram\n        ax2 = axes[1]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_proba = data['probabilities']\n            ax2.hist(y_proba, bins=20, alpha=0.7, color=colors[i], \n                    label=model_name, density=True)\n        \n        ax2.set_xlabel('Predicted Probability')\n        ax2.set_ylabel('Density')\n        ax2.set_title('Confidence Distribution')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/calibration_plots.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_confusion_matrices(self):\n        n_models = len(self.predictions)\n        cols = min(3, n_models)\n        rows = (n_models + cols - 1) // cols\n        \n        fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n        if n_models == 1:\n            axes = [axes]\n        elif rows == 1:\n            axes = [axes]\n        else:\n            axes = axes.flatten()\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_pred = data['predictions']\n            \n            cm = confusion_matrix(y_true, y_pred)\n            \n            # Normalize confusion matrix\n            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            \n            ax = axes[i] if n_models > 1 else axes[0]\n            \n            # Create heatmap\n            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n                       xticklabels=self.class_names, yticklabels=self.class_names,\n                       ax=ax, cbar_kws={'shrink': 0.8})\n            \n            ax.set_title(f'{model_name}\\nNormalized Confusion Matrix')\n            ax.set_ylabel('True Label')\n            ax.set_xlabel('Predicted Label')\n            \n            # Add counts\n            for j in range(cm.shape[0]):\n                for k in range(cm.shape[1]):\n                    ax.text(k+0.5, j+0.7, f'({cm[j,k]})', \n                           ha='center', va='center', fontsize=10, color='red')\n        \n        # Remove empty subplots\n        for j in range(n_models, len(axes)):\n            if j < len(axes):\n                fig.delaxes(axes[j])\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_threshold_analysis(self):\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        \n        # Threshold vs F1 Score\n        ax1 = axes[0, 0]\n        thresholds = np.arange(0.1, 1.0, 0.01)\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            f1_scores = []\n            for threshold in thresholds:\n                y_pred_thresh = (y_proba >= threshold).astype(int)\n                f1_scores.append(f1_score(y_true, y_pred_thresh))\n            \n            ax1.plot(thresholds, f1_scores, color=colors[i], label=model_name, linewidth=2)\n            \n            # Mark optimal threshold\n            optimal_idx = np.argmax(f1_scores)\n            ax1.scatter(thresholds[optimal_idx], f1_scores[optimal_idx], \n                       color=colors[i], s=100, marker='*', zorder=5)\n        \n        ax1.set_xlabel('Threshold')\n        ax1.set_ylabel('F1 Score')\n        ax1.set_title('F1 Score vs Threshold')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Threshold vs Precision/Recall\n        ax2 = axes[0, 1]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            precisions = []\n            recalls = []\n            for threshold in thresholds:\n                y_pred_thresh = (y_proba >= threshold).astype(int)\n                precisions.append(precision_score(y_true, y_pred_thresh, zero_division=0))\n                recalls.append(recall_score(y_true, y_pred_thresh, zero_division=0))\n            \n            ax2.plot(thresholds, precisions, color=colors[i], linestyle='-', \n                    label=f'{model_name} Precision', linewidth=2)\n            ax2.plot(thresholds, recalls, color=colors[i], linestyle='--', \n                    label=f'{model_name} Recall', linewidth=2)\n        \n        ax2.set_xlabel('Threshold')\n        ax2.set_ylabel('Score')\n        ax2.set_title('Precision/Recall vs Threshold')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        # Class distribution by confidence\n        ax3 = axes[1, 0]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            # Separate by true class\n            pos_probs = y_proba[y_true == 1]\n            neg_probs = y_proba[y_true == 0]\n            \n            ax3.hist(neg_probs, bins=20, alpha=0.5, color=colors[i], \n                    label=f'{model_name} Negative', density=True)\n            ax3.hist(pos_probs, bins=20, alpha=0.5, color=colors[i], \n                    label=f'{model_name} Positive', density=True, hatch='///')\n        \n        ax3.set_xlabel('Predicted Probability')\n        ax3.set_ylabel('Density')\n        ax3.set_title('Probability Distribution by True Class')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n        \n        # Sensitivity/Specificity vs Threshold\n        ax4 = axes[1, 1]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            sensitivities = []\n            specificities = []\n            for threshold in thresholds:\n                y_pred_thresh = (y_proba >= threshold).astype(int)\n                sensitivities.append(recall_score(y_true, y_pred_thresh, zero_division=0))\n                specificities.append(recall_score(1 - y_true, 1 - y_pred_thresh, zero_division=0))\n            \n            ax4.plot(thresholds, sensitivities, color=colors[i], linestyle='-', \n                    label=f'{model_name} Sensitivity', linewidth=2)\n            ax4.plot(thresholds, specificities, color=colors[i], linestyle='--', \n                    label=f'{model_name} Specificity', linewidth=2)\n        \n        ax4.set_xlabel('Threshold')\n        ax4.set_ylabel('Score')\n        ax4.set_title('Sensitivity/Specificity vs Threshold')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/threshold_analysis.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_prediction_distributions(self):\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        \n        # Box plots of predictions by true class\n        ax1 = axes[0, 0]\n        box_data = []\n        box_labels = []\n        \n        for model_name, data in self.predictions.items():\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            for class_idx, class_name in enumerate(self.class_names):\n                class_probs = y_proba[y_true == class_idx]\n                box_data.append(class_probs)\n                box_labels.append(f'{model_name}\\n{class_name}')\n        \n        ax1.boxplot(box_data, labels=box_labels)\n        ax1.set_title('Prediction Distribution by Model and True Class')\n        ax1.set_ylabel('Predicted Probability')\n        ax1.tick_params(axis='x', rotation=45)\n        ax1.grid(True, alpha=0.3)\n        \n        # Violin plots\n        ax2 = axes[0, 1]\n        positions = np.arange(1, len(box_data) + 1)\n        parts = ax2.violinplot(box_data, positions=positions, showmeans=True, showmedians=True)\n        ax2.set_xticks(positions)\n        ax2.set_xticklabels(box_labels, rotation=45)\n        ax2.set_title('Prediction Distribution (Violin Plot)')\n        ax2.set_ylabel('Predicted Probability')\n        ax2.grid(True, alpha=0.3)\n        \n        # Prediction confidence vs accuracy\n        ax3 = axes[1, 0]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_pred = data['predictions']\n            y_proba = data['probabilities']\n            \n            # Bin predictions by confidence\n            confidence_bins = np.linspace(0, 1, 11)\n            bin_accuracies = []\n            bin_centers = []\n            \n            for j in range(len(confidence_bins) - 1):\n                mask = (y_proba >= confidence_bins[j]) & (y_proba < confidence_bins[j + 1])\n                if mask.sum() > 0:\n                    bin_accuracy = (y_pred[mask] == y_true[mask]).mean()\n                    bin_accuracies.append(bin_accuracy)\n                    bin_centers.append((confidence_bins[j] + confidence_bins[j + 1]) / 2)\n            \n            ax3.plot(bin_centers, bin_accuracies, 'o-', color=colors[i], \n                    label=model_name, linewidth=2, markersize=6)\n        \n        ax3.set_xlabel('Prediction Confidence (Binned)')\n        ax3.set_ylabel('Accuracy')\n        ax3.set_title('Confidence vs Accuracy')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n        \n        # Error distribution\n        ax4 = axes[1, 1]\n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_pred = data['predictions']\n            \n            # Calculate error types\n            tp = ((y_true == 1) & (y_pred == 1)).sum()\n            tn = ((y_true == 0) & (y_pred == 0)).sum()\n            fp = ((y_true == 0) & (y_pred == 1)).sum()\n            fn = ((y_true == 1) & (y_pred == 0)).sum()\n            \n            categories = ['True Pos', 'True Neg', 'False Pos', 'False Neg']\n            values = [tp, tn, fp, fn]\n            \n            x_pos = np.arange(len(categories)) + i * 0.2\n            ax4.bar(x_pos, values, width=0.2, color=colors[i], \n                   label=model_name, alpha=0.8)\n        \n        ax4.set_xlabel('Prediction Type')\n        ax4.set_ylabel('Count')\n        ax4.set_title('Error Type Distribution')\n        ax4.set_xticks(np.arange(len(categories)) + 0.1)\n        ax4.set_xticklabels(categories)\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/prediction_distributions.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_error_analysis(self):\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            if i >= 6:  # Limit to 6 models for visualization\n                break\n                \n            y_true = data['labels']\n            y_pred = data['predictions']\n            y_proba = data['probabilities']\n            \n            ax = axes[i // 3, i % 3]\n            \n            # Create error analysis scatter plot\n            correct = (y_pred == y_true)\n            \n            # Plot correct predictions\n            ax.scatter(y_proba[correct], y_true[correct], \n                      alpha=0.6, c='green', label='Correct', s=20)\n            \n            # Plot incorrect predictions\n            ax.scatter(y_proba[~correct], y_true[~correct], \n                      alpha=0.6, c='red', label='Incorrect', s=20, marker='x')\n            \n            ax.set_xlabel('Predicted Probability')\n            ax.set_ylabel('True Label')\n            ax.set_title(f'{model_name} - Error Analysis')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n            \n            # Add decision boundary\n            ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n        \n        # Remove empty subplots\n        for j in range(len(self.predictions), 6):\n            axes[j // 3, j % 3].remove()\n        \n        plt.tight_layout()\n        plt.savefig(f'{self.save_dir}/error_analysis.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _create_interactive_plots(self):\n        print(\"Creating interactive visualizations...\")\n        \n        # Interactive ROC curve\n        fig_roc = go.Figure()\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n                auc = roc_auc_score(y_true, y_proba)\n                \n                fig_roc.add_trace(go.Scatter(\n                    x=fpr, y=tpr,\n                    mode='lines',\n                    name=f'{model_name} (AUC = {auc:.3f})',\n                    line=dict(width=3),\n                    hovertemplate='FPR: %{x:.3f}<br>TPR: %{y:.3f}<extra></extra>'\n                ))\n        \n        # Add diagonal line\n        fig_roc.add_trace(go.Scatter(\n            x=[0, 1], y=[0, 1],\n            mode='lines',\n            name='Random Classifier',\n            line=dict(dash='dash', color='gray', width=2)\n        ))\n        \n        fig_roc.update_layout(\n            title='Interactive ROC Curve Comparison',\n            xaxis_title='False Positive Rate',\n            yaxis_title='True Positive Rate',\n            width=800, height=600,\n            hovermode='closest'\n        )\n        \n        fig_roc.write_html(f'{self.save_dir}/interactive_roc.html')\n        \n        # Interactive threshold analysis\n        fig_threshold = make_subplots(\n            rows=2, cols=2,\n            subplot_titles=('F1 Score vs Threshold', 'Precision/Recall vs Threshold',\n                          'Sensitivity/Specificity vs Threshold', 'Sample Count vs Threshold')\n        )\n        \n        thresholds = np.arange(0.1, 1.0, 0.01)\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            # Calculate metrics for each threshold\n            f1_scores = []\n            precisions = []\n            recalls = []\n            specificities = []\n            sample_counts = []\n            \n            for threshold in thresholds:\n                y_pred_thresh = (y_proba >= threshold).astype(int)\n                f1_scores.append(f1_score(y_true, y_pred_thresh))\n                precisions.append(precision_score(y_true, y_pred_thresh, zero_division=0))\n                recalls.append(recall_score(y_true, y_pred_thresh, zero_division=0))\n                specificities.append(recall_score(1 - y_true, 1 - y_pred_thresh, zero_division=0))\n                sample_counts.append(y_pred_thresh.sum())\n            \n            # F1 Score\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=f1_scores, name=f'{model_name} F1',\n                          line=dict(width=2)),\n                row=1, col=1\n            )\n            \n            # Precision/Recall\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=precisions, name=f'{model_name} Precision',\n                          line=dict(width=2)),\n                row=1, col=2\n            )\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=recalls, name=f'{model_name} Recall',\n                          line=dict(width=2, dash='dash')),\n                row=1, col=2\n            )\n            \n            # Sensitivity\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=recalls, name=f'{model_name} Sensitivity',\n                          line=dict(width=2)),\n                row=2, col=1\n            )\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=specificities, name=f'{model_name} Specificity',\n                          line=dict(width=2, dash='dash')),\n                row=2, col=1\n            )\n            \n            # Sample count\n            fig_threshold.add_trace(\n                go.Scatter(x=thresholds, y=sample_counts, name=f'{model_name} Positive Predictions',\n                          line=dict(width=2)),\n                row=2, col=2\n            )\n        \n        fig_threshold.update_layout(\n            title='Interactive Threshold Analysis',\n            width=1200, height=800,\n            hovermode='x unified'\n        )\n        \n        fig_threshold.write_html(f'{self.save_dir}/interactive_threshold.html')\n        \n        # Interactive calibration plot\n        fig_cal = go.Figure()\n        \n        for i, (model_name, data) in enumerate(self.predictions.items()):\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n                \n                fig_cal.add_trace(go.Scatter(\n                    x=prob_pred, y=prob_true,\n                    mode='markers+lines',\n                    name=model_name,\n                    marker=dict(size=8),\n                    line=dict(width=3),\n                    hovertemplate='Predicted: %{x:.3f}<br>Actual: %{y:.3f}<extra></extra>'\n                ))\n        \n        # Perfect calibration line\n        fig_cal.add_trace(go.Scatter(\n            x=[0, 1], y=[0, 1],\n            mode='lines',\n            name='Perfect Calibration',\n            line=dict(dash='dash', color='gray', width=2)\n        ))\n        \n        fig_cal.update_layout(\n            title='Interactive Calibration Plot',\n            xaxis_title='Mean Predicted Probability',\n            yaxis_title='Fraction of Positives',\n            width=800, height=600\n        )\n        \n        fig_cal.write_html(f'{self.save_dir}/interactive_calibration.html')\n    \n    def _create_statistical_tests(self):\n        print(\"Performing statistical significance tests...\")\n        \n        if len(self.predictions) < 2:\n            print(\"Need at least 2 models for statistical comparison\")\n            return\n        \n        # McNemar's test for paired predictions\n        results_text = \"STATISTICAL SIGNIFICANCE TESTS\\n\" + \"=\"*50 + \"\\n\\n\"\n        \n        model_names = list(self.predictions.keys())\n        n_models = len(model_names)\n        \n        # Create comparison matrix\n        comparison_results = pd.DataFrame(index=model_names, columns=model_names)\n        \n        for i in range(n_models):\n            for j in range(i+1, n_models):\n                model1_name = model_names[i]\n                model2_name = model_names[j]\n                \n                y_true = self.predictions[model1_name]['labels']\n                pred1 = self.predictions[model1_name]['predictions']\n                pred2 = self.predictions[model2_name]['predictions']\n                \n                # McNemar's test\n                # Create contingency table\n                correct1 = (pred1 == y_true)\n                correct2 = (pred2 == y_true)\n                \n                both_correct = np.sum(correct1 & correct2)\n                model1_only = np.sum(correct1 & ~correct2)\n                model2_only = np.sum(~correct1 & correct2)\n                both_wrong = np.sum(~correct1 & ~correct2)\n                \n                # McNemar's test statistic\n                if (model1_only + model2_only) > 0:\n                    mcnemar_stat = (abs(model1_only - model2_only) - 1)**2 / (model1_only + model2_only)\n                    p_value = 1 - stats.chi2.cdf(mcnemar_stat, 1)\n                else:\n                    mcnemar_stat = 0\n                    p_value = 1.0\n                \n                comparison_results.loc[model1_name, model2_name] = f\"p={p_value:.4f}\"\n                comparison_results.loc[model2_name, model1_name] = f\"Ï‡Â²={mcnemar_stat:.4f}\"\n                \n                results_text += f\"McNemar's Test: {model1_name} vs {model2_name}\\n\"\n                results_text += f\"  Contingency Table:\\n\"\n                results_text += f\"    Both correct: {both_correct}\\n\"\n                results_text += f\"    {model1_name} only: {model1_only}\\n\"\n                results_text += f\"    {model2_name} only: {model2_only}\\n\"\n                results_text += f\"    Both wrong: {both_wrong}\\n\"\n                results_text += f\"  Chi-square statistic: {mcnemar_stat:.4f}\\n\"\n                results_text += f\"  P-value: {p_value:.4f}\\n\"\n                \n                if p_value < 0.05:\n                    results_text += f\"  Result: Significant difference (p < 0.05)\\n\"\n                else:\n                    results_text += f\"  Result: No significant difference (p >= 0.05)\\n\"\n                results_text += \"\\n\"\n        \n        # Bootstrap confidence intervals for AUC\n        results_text += \"BOOTSTRAP CONFIDENCE INTERVALS (AUC)\\n\" + \"-\"*40 + \"\\n\"\n        \n        for model_name, data in self.predictions.items():\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                # Bootstrap sampling\n                n_bootstrap = 1000\n                bootstrap_aucs = []\n                \n                for _ in range(n_bootstrap):\n                    # Sample with replacement\n                    indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n                    y_true_boot = y_true[indices]\n                    y_proba_boot = y_proba[indices]\n                    \n                    if len(np.unique(y_true_boot)) > 1:\n                        auc_boot = roc_auc_score(y_true_boot, y_proba_boot)\n                        bootstrap_aucs.append(auc_boot)\n                \n                if bootstrap_aucs:\n                    ci_lower = np.percentile(bootstrap_aucs, 2.5)\n                    ci_upper = np.percentile(bootstrap_aucs, 97.5)\n                    mean_auc = np.mean(bootstrap_aucs)\n                    \n                    results_text += f\"{model_name}:\\n\"\n                    results_text += f\"  Mean AUC: {mean_auc:.4f}\\n\"\n                    results_text += f\"  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\\n\\n\"\n        \n        # Save results\n        with open(f'{self.save_dir}/statistical_tests.txt', 'w') as f:\n            f.write(results_text)\n        \n        # Create visualization of statistical tests\n        plt.figure(figsize=(10, 8))\n        \n        # Plot AUC with confidence intervals\n        model_names = []\n        auc_values = []\n        ci_lowers = []\n        ci_uppers = []\n        \n        for model_name, data in self.predictions.items():\n            y_true = data['labels']\n            y_proba = data['probabilities']\n            \n            if len(np.unique(y_true)) > 1:\n                # Bootstrap for CI\n                n_bootstrap = 1000\n                bootstrap_aucs = []\n                \n                for _ in range(n_bootstrap):\n                    indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n                    y_true_boot = y_true[indices]\n                    y_proba_boot = y_proba[indices]\n                    \n                    if len(np.unique(y_true_boot)) > 1:\n                        auc_boot = roc_auc_score(y_true_boot, y_proba_boot)\n                        bootstrap_aucs.append(auc_boot)\n                \n                if bootstrap_aucs:\n                    model_names.append(model_name)\n                    auc_values.append(np.mean(bootstrap_aucs))\n                    ci_lowers.append(np.percentile(bootstrap_aucs, 2.5))\n                    ci_uppers.append(np.percentile(bootstrap_aucs, 97.5))\n        \n        if model_names:\n            y_pos = np.arange(len(model_names))\n            \n            plt.errorbar(auc_values, y_pos, \n                        xerr=[np.array(auc_values) - np.array(ci_lowers), \n                              np.array(ci_uppers) - np.array(auc_values)],\n                        fmt='o', capsize=5, capthick=2, markersize=8)\n            \n            plt.yticks(y_pos, model_names)\n            plt.xlabel('AUC Score')\n            plt.title('Model Performance with 95% Bootstrap Confidence Intervals')\n            plt.grid(True, alpha=0.3)\n            \n            # Add vertical line at 0.5 (random performance)\n            plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n            plt.legend()\n            \n            plt.tight_layout()\n            plt.savefig(f'{self.save_dir}/statistical_comparison.png', dpi=300, bbox_inches='tight')\n            plt.close()\n    \n    def generate_comprehensive_report(self):\n        print(\"Generating comprehensive report...\")\n        \n        html_content = \"\"\"\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <title>Model Evaluation Report</title>\n            <style>\n                body { font-family: Arial, sans-serif; margin: 20px; }\n                .header { background-color: #f0f0f0; padding: 20px; border-radius: 10px; }\n                .section { margin: 20px 0; padding: 15px; border-left: 4px solid #007acc; }\n                .metrics-table { border-collapse: collapse; width: 100%; margin: 20px 0; }\n                .metrics-table th, .metrics-table td { border: 1px solid #ddd; padding: 8px; text-align: center; }\n                .metrics-table th { background-color: #f2f2f2; }\n                .image-container { text-align: center; margin: 20px 0; }\n                .best-score { background-color: #90EE90; }\n                .worst-score { background-color: #FFB6C1; }\n            </style>\n        </head>\n        <body>\n        \"\"\"\n        \n        # Header\n        html_content += f\"\"\"\n        <div class=\"header\">\n            <h1>Advanced Model Evaluation Report</h1>\n            <p><strong>Generated on:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n            <p><strong>Number of Models:</strong> {len(self.predictions)}</p>\n            <p><strong>Dataset Size:</strong> {len(list(self.predictions.values())[0]['labels'])}</p>\n        </div>\n        \"\"\"\n        \n        # Executive Summary\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Executive Summary</h2>\n        \"\"\"\n        \n        # Find best performing model\n        best_model = None\n        best_auc = 0\n        \n        for model_name in self.results:\n            if self.results[model_name]['roc_auc'] > best_auc:\n                best_auc = self.results[model_name]['roc_auc']\n                best_model = model_name\n        \n        if best_model:\n            html_content += f\"\"\"\n            <p><strong>Best Performing Model:</strong> {best_model}</p>\n            <p><strong>Best AUC Score:</strong> {best_auc:.4f}</p>\n            <p><strong>Best F1 Score:</strong> {self.results[best_model]['f1_score']:.4f}</p>\n            <p><strong>Best Accuracy:</strong> {self.results[best_model]['accuracy']:.4f}</p>\n            \"\"\"\n        \n        html_content += \"</div>\"\n        \n        # Detailed Metrics Table\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Detailed Performance Metrics</h2>\n            <table class=\"metrics-table\">\n                <tr>\n                    <th>Model</th>\n                    <th>Accuracy</th>\n                    <th>Precision</th>\n                    <th>Recall</th>\n                    <th>F1 Score</th>\n                    <th>AUC</th>\n                    <th>Specificity</th>\n                    <th>Matthews Corr</th>\n                </tr>\n        \"\"\"\n        \n        # Find best/worst for each metric\n        metrics_to_highlight = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'specificity', 'matthews_corr']\n        best_values = {}\n        worst_values = {}\n        \n        for metric in metrics_to_highlight:\n            values = [self.results[model][metric] for model in self.results]\n            best_values[metric] = max(values)\n            worst_values[metric] = min(values)\n        \n        for model_name in self.results:\n            html_content += f\"<tr><td><strong>{model_name}</strong></td>\"\n            \n            for metric in metrics_to_highlight:\n                value = self.results[model_name][metric]\n                css_class = \"\"\n                \n                if value == best_values[metric] and len(self.results) > 1:\n                    css_class = \"best-score\"\n                elif value == worst_values[metric] and len(self.results) > 1:\n                    css_class = \"worst-score\"\n                \n                html_content += f'<td class=\"{css_class}\">{value:.4f}</td>'\n            \n            html_content += \"</tr>\"\n        \n        html_content += \"</table></div>\"\n        \n        # Visualizations\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Visualizations</h2>\n        \"\"\"\n        \n        # List all generated plots\n        plot_files = [\n            'performance_dashboard.png',\n            'roc_comparison.png', \n            'pr_comparison.png',\n            'calibration_plots.png',\n            'confusion_matrices.png',\n            'threshold_analysis.png',\n            'prediction_distributions.png',\n            'error_analysis.png',\n            'statistical_comparison.png'\n        ]\n        \n        for plot_file in plot_files:\n            if os.path.exists(f'{self.save_dir}/{plot_file}'):\n                html_content += f\"\"\"\n                <div class=\"image-container\">\n                    <h3>{plot_file.replace('_', ' ').replace('.png', '').title()}</h3>\n                    <img src=\"{plot_file}\" style=\"max-width: 100%; height: auto;\" alt=\"{plot_file}\">\n                </div>\n                \"\"\"\n        \n        html_content += \"</div>\"\n        \n        # Interactive Plots\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Interactive Visualizations</h2>\n            <p>The following interactive plots have been generated:</p>\n            <ul>\n                <li><a href=\"interactive_roc.html\">Interactive ROC Curves</a></li>\n                <li><a href=\"interactive_threshold.html\">Interactive Threshold Analysis</a></li>\n                <li><a href=\"interactive_calibration.html\">Interactive Calibration Plot</a></li>\n            </ul>\n        </div>\n        \"\"\"\n        \n        # Model Recommendations\n        html_content += \"\"\"\n        <div class=\"section\">\n            <h2>Recommendations</h2>\n        \"\"\"\n        \n        if best_model and self.results[best_model]['roc_auc'] > 0.8:\n            html_content += f\"<p><strong>{best_model}</strong> shows excellent performance with AUC > 0.8. This model is ready for deployment consideration.</p>\"\n        elif best_model and self.results[best_model]['roc_auc'] > 0.7:\n            html_content += f\"<p><strong>{best_model}</strong> shows good performance with AUC > 0.7, but there's room for improvement.</p>\"\n        else:\n            html_content += \"<p>All models show suboptimal performance (AUC < 0.7). Consider:</p>\"\n            html_content += \"<ul><li>Collecting more training data</li><li>Feature engineering</li><li>Different model architectures</li><li>Hyperparameter tuning</li></ul>\"\n        \n        # Check for overfitting signs\n        for model_name in self.results:\n            if 'calibration_error' in self.results[model_name]:\n                cal_error = self.results[model_name]['calibration_error']\n                if cal_error > 0.1:\n                    html_content += f\"<p><strong>{model_name}</strong> shows poor calibration (error: {cal_error:.3f}). Consider calibration techniques.</p>\"\n        \n        html_content += \"</div>\"\n        \n        html_content += \"\"\"\n        </body>\n        </html>\n        \"\"\"\n        \n        # Save HTML report\n        with open(f'{self.save_dir}/comprehensive_report.html', 'w') as f:\n            f.write(html_content)\n        \n        print(f\"Comprehensive report saved to {self.save_dir}/comprehensive_report.html\")\n    \n    def run_complete_evaluation(self, model_class):\n        print(\"Starting comprehensive model evaluation...\")\n        print(\"=\"*60)\n        \n        # Load models and generate predictions\n        self.load_model_predictions(model_class)\n        \n        if not self.predictions:\n            print(\"No valid predictions generated. Check model paths and data loader.\")\n            return None\n        \n        # Calculate comprehensive metrics\n        self.calculate_comprehensive_metrics()\n        \n        # Create visualizations\n        self.create_comprehensive_visualizations()\n        \n        # Generate report\n        self.generate_comprehensive_report()\n        \n        # Save results to JSON\n        results_for_json = {}\n        for model_name in self.results:\n            results_for_json[model_name] = {}\n            for metric, value in self.results[model_name].items():\n                if isinstance(value, (int, float, np.integer, np.floating)):\n                    results_for_json[model_name][metric] = float(value)\n                else:\n                    results_for_json[model_name][metric] = str(value)\n        \n        with open(f'{self.save_dir}/detailed_results.json', 'w') as f:\n            json.dump(results_for_json, f, indent=2)\n        \n        print(f\"\\nEvaluation completed successfully!\")\n        print(f\"Results saved in: {self.save_dir}/\")\n        print(f\"Open {self.save_dir}/comprehensive_report.html for full report\")\n        \n        return self.results\n\n\n# Usage Example\ndef run_evaluation_pipeline(model_paths, data_loader, model_class, device, save_dir=\"evaluation_results\"):\n    \"\"\"\n    Run the complete evaluation pipeline\n    \n    Args:\n        model_paths: List of model checkpoint paths or single path\n        data_loader: PyTorch DataLoader with test/validation data\n        model_class: Model class (not instantiated)\n        device: torch.device\n        save_dir: Directory to save results\n    \n    Returns:\n        Dictionary with evaluation results\n    \"\"\"\n    \n    # Create evaluator\n    evaluator = AdvancedModelEvaluator(\n        model_paths=model_paths,\n        data_loader=data_loader,\n        device=device,\n        class_names=['No Aneurysm', 'Aneurysm'],\n        save_dir=save_dir\n    )\n    \n    # Run complete evaluation\n    results = evaluator.run_complete_evaluation(model_class)\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}