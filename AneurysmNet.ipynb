{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**IMPORTS AND TPU SET UP**","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\n\ndef install_required_packages():\n    packages = [\n        'pydicom',\n        'nibabel', \n        'opencv-python',\n        'scikit-learn'\n    ]\n    \n    for package in packages:\n        try:\n            __import__(package.replace('-', '_'))\n            print(f\"{package} already installed\")\n        except ImportError:\n            print(f\"Installing {package}...\")\n            try:\n                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n                print(f\"{package} installed successfully\")\n            except Exception as e:\n                print(f\"‚ö† Failed to install {package}: {e}\")\n\nprint(\"Checking and installing required packages...\")\ninstall_required_packages()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pydicom\nfrom pydicom.errors import InvalidDicomError\nimport nibabel as nib\nimport cv2\nfrom scipy import ndimage\nfrom tqdm.auto import tqdm\nimport warnings\nimport gc\nimport time\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings('ignore')\n\n# TPU-SPECIFIC SETUP (Fixed)\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    TPU_AVAILABLE = True\n    print(\"TPU libraries loaded successfully\")\nexcept ImportError:\n    TPU_AVAILABLE = False\n    print(\"TPU libraries not available, falling back to GPU/CPU\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TPU OPTMIZED CONFIGURATION AND TRAINING PIPELINE**","metadata":{}},{"cell_type":"code","source":"class Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    \n    # Model parameters (Fixed: More reasonable sizes)\n    TARGET_SIZE = (64, 128, 128)  # Depth, Height, Width\n    BATCH_SIZE = 2 if TPU_AVAILABLE else 4\n    EPOCHS = 10\n    LEARNING_RATE = 1e-4\n    WEIGHT_DECAY = 1e-4\n    \n    # Device setup\n    if TPU_AVAILABLE:\n        DEVICE = xm.xla_device()\n        print(f\"Using TPU device: {DEVICE}\")\n    else:\n        DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {DEVICE}\")\n    \n    # Data parameters\n    ID_COL = 'SeriesInstanceUID'\n    TARGET_COL = 'Aneurysm Present'\n    \n    # Debug settings\n    DEBUG_MODE = True\n    DEBUG_SAMPLES = 200\n\n# Fixed: Proper 3D CNN Architecture for Classification Only\nclass ImprovedAneurysmNet(nn.Module):\n    def __init__(self, in_channels=1, num_classes=1, dropout_rate=0.3):\n        super(ImprovedAneurysmNet, self).__init__()\n        \n        # Feature extraction layers\n        self.features = nn.Sequential(\n            # First block\n            nn.Conv3d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool3d(kernel_size=2, stride=2),\n            nn.Dropout3d(dropout_rate * 0.5),\n            \n            # Second block\n            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool3d(kernel_size=2, stride=2),\n            nn.Dropout3d(dropout_rate * 0.7),\n            \n            # Third block\n            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool3d(kernel_size=2, stride=2),\n            nn.Dropout3d(dropout_rate),\n        )\n        \n        # Adaptive pooling to handle variable input sizes\n        self.adaptive_pool = nn.AdaptiveAvgPool3d((2, 4, 4))\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(128 * 2 * 4 * 4, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate * 0.7),\n            nn.Linear(256, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate * 0.5),\n            nn.Linear(128, num_classes)\n        )\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.adaptive_pool(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.classifier(x)\n        return x\n\n# Fixed: Better DICOM processor\nclass DICOMProcessor:\n    def __init__(self, target_size=None):\n        self.target_size = target_size or Config.TARGET_SIZE\n        self.max_slices = self.target_size[0]\n        \n    def load_dicom_series(self, series_path):\n        try:\n            if not os.path.exists(series_path):\n                return self._get_dummy_volume()\n                \n            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if not dicom_files:\n                return self._get_dummy_volume()\n            \n            # Sort files by instance number or filename\n            dicom_files.sort()\n            \n            # Select evenly distributed slices\n            if len(dicom_files) > self.max_slices:\n                indices = np.linspace(0, len(dicom_files)-1, self.max_slices, dtype=int)\n                selected_files = [dicom_files[i] for i in indices]\n            else:\n                selected_files = dicom_files\n            \n            pixel_arrays = []\n            target_shape = self.target_size[1:]  # H, W\n            \n            for file_name in selected_files:\n                try:\n                    ds = pydicom.dcmread(os.path.join(series_path, file_name), force=True)\n                    \n                    if hasattr(ds, 'pixel_array'):\n                        arr = ds.pixel_array.astype(np.float32)\n                        \n                        if arr.ndim == 2:\n                            # Resize to target shape\n                            if arr.shape != target_shape:\n                                arr = cv2.resize(arr, (target_shape[1], target_shape[0]), \n                                               interpolation=cv2.INTER_LINEAR)\n                            pixel_arrays.append(arr)\n                        elif arr.ndim == 3:\n                            # Take middle slice for multi-slice files\n                            middle_slice = arr[arr.shape[0] // 2]\n                            if middle_slice.shape != target_shape:\n                                middle_slice = cv2.resize(middle_slice, (target_shape[1], target_shape[0]), \n                                                        interpolation=cv2.INTER_LINEAR)\n                            pixel_arrays.append(middle_slice)\n                    \n                    del ds\n                    \n                except Exception as e:\n                    continue\n            \n            if not pixel_arrays:\n                return self._get_dummy_volume()\n            \n            # Pad or truncate to exact number of slices\n            while len(pixel_arrays) < self.max_slices:\n                if pixel_arrays:\n                    pixel_arrays.append(pixel_arrays[-1])  # Repeat last slice\n                else:\n                    pixel_arrays.append(np.zeros(target_shape, dtype=np.float32))\n            \n            if len(pixel_arrays) > self.max_slices:\n                pixel_arrays = pixel_arrays[:self.max_slices]\n            \n            # Stack into volume\n            volume = np.stack(pixel_arrays, axis=0).astype(np.float32)\n            \n            # Preprocessing\n            volume = self._preprocess_volume(volume)\n            \n            return volume\n            \n        except Exception as e:\n            print(f\"Error processing {series_path}: {e}\")\n            return self._get_dummy_volume()\n    \n    def _get_dummy_volume(self):\n        return np.random.normal(0.5, 0.1, self.target_size).astype(np.float32)\n    \n    def _preprocess_volume(self, volume):\n        # Robust normalization using percentiles\n        p1, p99 = np.percentile(volume, [1, 99])\n        if p99 > p1:\n            volume = np.clip(volume, p1, p99)\n            volume = (volume - p1) / (p99 - p1)\n        else:\n            volume = np.zeros_like(volume)\n        \n        # Additional normalization\n        volume = np.clip(volume, 0, 1).astype(np.float32)\n        \n        return volume\n\n# Fixed: Improved Dataset\nclass AneurysmDataset(Dataset):\n    def __init__(self, df, series_dir, processor, transform=None):\n        self.df = df.copy().reset_index(drop=True)\n        self.series_dir = series_dir\n        self.processor = processor\n        self.transform = transform\n        \n        print(f\"Dataset created with {len(self.df)} samples\")\n        print(f\"Positive cases: {self.df[Config.TARGET_COL].sum()}\")\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        try:\n            row = self.df.iloc[idx]\n            series_id = row[Config.ID_COL]\n            label = float(row[Config.TARGET_COL])\n            \n            series_path = os.path.join(self.series_dir, series_id)\n            \n            # Load volume\n            volume = self.processor.load_dicom_series(series_path)\n            \n            # Add channel dimension\n            volume_tensor = torch.from_numpy(volume).float().unsqueeze(0)\n            label_tensor = torch.tensor(label, dtype=torch.float32)\n            \n            return {\n                'volume': volume_tensor,\n                'label': label_tensor,\n                'series_id': series_id\n            }\n            \n        except Exception as e:\n            # Return dummy data on failure\n            return {\n                'volume': torch.zeros((1, *Config.TARGET_SIZE), dtype=torch.float32),\n                'label': torch.tensor(0.0, dtype=torch.float32),\n                'series_id': f\"DUMMY_{idx}\"\n            }\n\n# Fixed: Proper loss function with class balancing\nclass BalancedBCELoss(nn.Module):\n    def __init__(self, pos_weight=None):\n        super(BalancedBCELoss, self).__init__()\n        self.pos_weight = pos_weight\n        \n    def forward(self, input, target):\n        if self.pos_weight is not None:\n            loss = nn.functional.binary_cross_entropy_with_logits(\n                input.view(-1), target, pos_weight=self.pos_weight\n            )\n        else:\n            loss = nn.functional.binary_cross_entropy_with_logits(\n                input.view(-1), target\n            )\n        return loss\n\n# Fixed: Training functions\ndef train_epoch(model, loader, optimizer, criterion, device, epoch):\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    \n    progress_bar = tqdm(\n        enumerate(loader),\n        total=len(loader),\n        desc=f\"Training Epoch {epoch+1}\",\n        leave=False\n    )\n    \n    for batch_idx, batch in progress_bar:\n        volume = batch['volume'].to(device)\n        label = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(volume)\n        loss = criterion(outputs, label)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        # Optimizer step\n        if TPU_AVAILABLE:\n            xm.optimizer_step(optimizer)\n        else:\n            optimizer.step()\n        \n        total_loss += loss.item()\n        num_batches += 1\n        \n        progress_bar.set_postfix({\n            'Loss': f'{loss.item():.4f}',\n            'Avg': f'{total_loss/num_batches:.4f}'\n        })\n        \n        # TPU synchronization\n        if TPU_AVAILABLE and batch_idx % 10 == 0:\n            xm.mark_step()\n    \n    progress_bar.close()\n    \n    if TPU_AVAILABLE:\n        xm.mark_step()\n    \n    return total_loss / max(num_batches, 1)\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    progress_bar = tqdm(\n        enumerate(loader),\n        total=len(loader),\n        desc=\"Validation\",\n        leave=False\n    )\n    \n    with torch.no_grad():\n        for batch_idx, batch in progress_bar:\n            volume = batch['volume'].to(device)\n            label = batch['label'].to(device)\n            \n            outputs = model(volume)\n            loss = criterion(outputs, label)\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Get predictions\n            probs = torch.sigmoid(outputs).cpu().numpy()\n            preds = (probs > 0.5).astype(int)\n            \n            all_probs.extend(probs.flatten())\n            all_preds.extend(preds.flatten())\n            all_labels.extend(label.cpu().numpy())\n            \n            progress_bar.set_postfix({\n                'Val Loss': f'{loss.item():.4f}',\n                'Avg': f'{total_loss/num_batches:.4f}'\n            })\n    \n    progress_bar.close()\n    \n    if TPU_AVAILABLE:\n        xm.mark_step()\n    \n    avg_loss = total_loss / max(num_batches, 1)\n    accuracy = accuracy_score(all_labels, all_preds) if len(all_preds) > 0 else 0\n    \n    return avg_loss, accuracy, all_preds, all_labels, all_probs\n\n# Fixed: Main training function\ndef main_training():\n    print(\"üß† ANEURYSM DETECTION TRAINING\")\n    print(\"=\"*50)\n    print(f\"Device: {Config.DEVICE}\")\n    print(f\"TPU Available: {TPU_AVAILABLE}\")\n    \n    # Load data\n    print(\"\\nüìä Loading training data...\")\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    \n    if Config.DEBUG_MODE:\n        train_df = train_df.head(Config.DEBUG_SAMPLES)\n        print(f\"üîç Debug mode: using {len(train_df)} samples\")\n    \n    print(f\"Training samples: {len(train_df)}\")\n    print(f\"Positive cases: {train_df[Config.TARGET_COL].sum()}\")\n    print(f\"Negative cases: {len(train_df) - train_df[Config.TARGET_COL].sum()}\")\n    \n    # Calculate class weights for balanced training\n    pos_count = train_df[Config.TARGET_COL].sum()\n    neg_count = len(train_df) - pos_count\n    pos_weight = torch.tensor([neg_count / pos_count if pos_count > 0 else 1.0])\n    print(f\"Positive weight: {pos_weight.item():.2f}\")\n    \n    # Train/validation split\n    train_data, val_data = train_test_split(\n        train_df, test_size=0.2, random_state=42, \n        stratify=train_df[Config.TARGET_COL]\n    )\n    \n    print(f\"üìä Train: {len(train_data)}, Val: {len(val_data)}\")\n    \n    # Create datasets\n    processor = DICOMProcessor()\n    train_dataset = AneurysmDataset(train_data, Config.SERIES_DIR, processor)\n    val_dataset = AneurysmDataset(val_data, Config.SERIES_DIR, processor)\n    \n    # Data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=Config.BATCH_SIZE,\n        shuffle=True,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=Config.BATCH_SIZE,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=False\n    )\n    \n    print(f\"üì¶ Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n    \n    # Create model\n    print(\"\\nü§ñ Creating model...\")\n    model = ImprovedAneurysmNet().to(Config.DEVICE)\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Model parameters: {total_params:,} (trainable: {trainable_params:,})\")\n    \n    # Loss function and optimizer\n    criterion = BalancedBCELoss(pos_weight=pos_weight.to(Config.DEVICE))\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=Config.LEARNING_RATE,\n        weight_decay=Config.WEIGHT_DECAY\n    )\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n    )\n    \n    # Training loop\n    best_val_loss = float('inf')\n    best_val_acc = 0\n    patience_counter = 0\n    patience_limit = 5\n    \n    print(f\"\\nüöÄ Starting training for {Config.EPOCHS} epochs...\")\n    \n    for epoch in range(Config.EPOCHS):\n        print(f\"\\n{'='*15} EPOCH {epoch+1}/{Config.EPOCHS} {'='*15}\")\n        \n        # Training\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, Config.DEVICE, epoch)\n        \n        # Validation\n        val_loss, val_acc, val_preds, val_labels, val_probs = validate_epoch(\n            model, val_loader, criterion, Config.DEVICE\n        )\n        \n        # Update scheduler\n        scheduler.step(val_loss)\n        \n        print(f\"\\nüìä Epoch {epoch+1} Results:\")\n        print(f\"   Train Loss: {train_loss:.4f}\")\n        print(f\"   Val Loss: {val_loss:.4f}\")\n        print(f\"   Val Accuracy: {val_acc:.4f}\")\n        print(f\"   LR: {optimizer.param_groups[0]['lr']:.2e}\")\n        \n        # Calculate additional metrics\n        if len(set(val_labels)) > 1 and len(val_probs) > 0:\n            try:\n                auc = roc_auc_score(val_labels, val_probs)\n                print(f\"   Val AUC: {auc:.4f}\")\n            except:\n                pass\n        \n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_val_acc = val_acc\n            patience_counter = 0\n            \n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'val_accuracy': val_acc,\n                'best_val_loss': best_val_loss\n            }\n            \n            torch.save(checkpoint, 'tpu_aneurysm_best.pth')\n            print(f\"üíæ Saved best model (val_loss: {val_loss:.4f}, acc: {val_acc:.4f})\")\n        else:\n            patience_counter += 1\n            print(f\"‚è≥ Patience: {patience_counter}/{patience_limit}\")\n        \n        # Early stopping\n        if patience_counter >= patience_limit:\n            print(f\"‚ÑπÔ∏è Early stopping at epoch {epoch+1}\")\n            break\n        \n        # Memory cleanup\n        if not TPU_AVAILABLE:\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    print(f\"\\n‚úÖ Training completed!\")\n    print(f\"üèÜ Best validation loss: {best_val_loss:.4f}\")\n    print(f\"üèÜ Best validation accuracy: {best_val_acc:.4f}\")\n    \n    return model, best_val_loss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# RUN TRAINING\nprint(\"Starting TPU-optimized training...\")\nmodel, best_loss = main_tpu_training()\n\nif model is not None:\n    print(f\"\\nTraining completed successfully!\")\n    print(f\"Best validation loss: {best_loss:.4f}\")\n    print(\"Output files: tpu_aneurysm_final.pth, tpu_aneurysm_best.pth\")\nelse:\n    print(\"Training failed. Check error messages above.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model Evaluation and Performance**","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model_path='tpu_aneurysm_best.pth'):\n    print(\"üîç MODEL EVALUATION\")\n    print(\"=\"*50)\n    \n    # Load test data\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    test_size = min(100, len(train_df) // 5)\n    test_df = train_df.sample(n=test_size, random_state=123).reset_index(drop=True)\n    \n    print(f\"üìä Test set: {len(test_df)} samples\")\n    print(f\"‚ûï Positive cases: {test_df[Config.TARGET_COL].sum()}\")\n    \n    # Load model\n    model = ImprovedAneurysmNet().to(Config.DEVICE)\n    \n    if os.path.exists(model_path):\n        print(f\"üìÅ Loading model from {model_path}\")\n        checkpoint = torch.load(model_path, map_location='cpu')\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model = model.to(Config.DEVICE)\n        print(f\"‚úÖ Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n        print(f\"üèÜ Best val loss: {checkpoint.get('best_val_loss', 'unknown')}\")\n    else:\n        print(f\"‚ö†Ô∏è Model file not found, using random weights\")\n    \n    # Create test dataset\n    processor = DICOMProcessor()\n    test_dataset = AneurysmDataset(test_df, Config.SERIES_DIR, processor)\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=2,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=False\n    )\n    \n    # Evaluation\n    model.eval()\n    predictions = []\n    probabilities = []\n    true_labels = []\n    \n    print(\"üîÑ Running evaluation...\")\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            volume = batch['volume'].to(Config.DEVICE)\n            label = batch['label']\n            \n            outputs = model(volume)\n            probs = torch.sigmoid(outputs).cpu().numpy()\n            preds = (probs > 0.5).astype(int)\n            \n            probabilities.extend(probs.flatten())\n            predictions.extend(preds.flatten())\n            true_labels.extend(label.numpy())\n    \n    # Calculate metrics\n    predictions = np.array(predictions)\n    probabilities = np.array(probabilities)\n    true_labels = np.array(true_labels)\n    \n    if len(predictions) > 0:\n        accuracy = accuracy_score(true_labels, predictions)\n        \n        print(f\"\\nüìä EVALUATION RESULTS\")\n        print(f\"{'='*30}\")\n        print(f\"üéØ Accuracy: {accuracy:.3f}\")\n        print(f\"üìà Predictions made: {len(predictions)}\")\n        \n        if len(np.unique(true_labels)) > 1:\n            try:\n                auc = roc_auc_score(true_labels, probabilities)\n                print(f\"üìà AUC-ROC: {auc:.3f}\")\n            except:\n                print(\"‚ö†Ô∏è Could not calculate AUC\")\n        \n        print(f\"\\nüìã Classification Report:\")\n        print(classification_report(true_labels, predictions, \n                                  target_names=['No Aneurysm', 'Aneurysm'],\n                                  zero_division=0))\n        \n        # Sample predictions\n        print(f\"\\nüîé Sample Predictions:\")\n        sample_size = min(10, len(predictions))\n        for i in range(sample_size):\n            status = \"‚úÖ\" if predictions[i] == true_labels[i] else \"‚ùå\"\n            print(f\"{status} True: {int(true_labels[i])}, \"\n                  f\"Pred: {int(predictions[i])}, \"\n                  f\"Prob: {probabilities[i]:.3f}\")\n    \n    print(f\"\\n‚úÖ Evaluation completed!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"üîç RUNNING MODEL EVALUATION\")\nprint(\"=\"*60)\n\neval_results = quick_tpu_evaluation()","metadata":{"trusted":true},"outputs":[{"name":"stderr","text":"https://symbolize.stripped_domain/r/?trace=7f4798bb7ee6,7f4798aeb04f&map= \n*** SIGTERM received by PID 10 (TID 10) on cpu 2 from PID 1; stack trace: ***\nPC: @     0x7f4798bb7ee6  (unknown)  epoll_wait\n    @     0x7f434efa9a01       1888  (unknown)\n    @     0x7f4798aeb050  (unknown)  (unknown)\nhttps://symbolize.stripped_domain/r/?trace=7f4798bb7ee6,7f434efa9a00,7f4798aeb04f&map= \nE0821 01:07:58.377615      10 coredump_hook.cc:247] RAW: Remote crash gathering disabled for SIGTERM.\n","output_type":"stream"}],"execution_count":null}]}